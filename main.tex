\documentclass[
% -- opções da classe memoir --
%oldfontcommands,
12pt,				% tamanho da fonte
openright,			% capítulos começam em pág ímpar (insere página vazia caso preciso)
oneside,			% para impressão em verso e anverso. Oposto a oneside
a4paper,			% tamanho do papel.
% -- opções da classe abntex2 --
%chapter=TITLE,		% títulos de capítulos convertidos em letras maiúsculas
%section=TITLE,		% títulos de seções convertidos em letras maiúsculas
%subsection=TITLE,	% títulos de subseções convertidos em letras maiúsculas
%subsubsection=TITLE,% títulos de subsubseções convertidos em letras maiúsculas
% -- opções do pacote babel --
english,			% idioma adicional para hifenização
french,				% idioma adicional para hifenização
spanish,			% idioma adicional para hifenização
brasil,				% o último idioma é o principal do documento
]{abntex2}

% ---
% PACOTES
% ---
% ---
% Pacotes fundamentais
% ---
\usepackage{cmap}				% Mapear caracteres especiais no PDF
%\usepackage{lmodern}			% Usa a fonte Latin Modern
\usepackage{helvet}
\usepackage[T1]{fontenc}		% Selecao de codigos de fonte.
\usepackage[utf8]{inputenc}		% Codificacao do documento (conversão automática dos acentos)
\usepackage{lastpage}			% Usado pela Ficha catalográfica
\usepackage{indentfirst}		% Indenta o primeiro parágrafo de cada seção.
\usepackage{color}				% Controle das cores
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{underscore}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{graphicx} % Usado para outros tipos de imagens
\usepackage{float} % Usado para posicionamento de imagens
\usepackage[notransparent]{svg}  % Eis o pacote que queremos.
\usepackage{alltt}
\usepackage{filecontents}
\usepackage{listings}
\usepackage{subfiles}
\linespread{1.5} % espaçamento entre linhas
% ---
% Pacotes adicionais, usados apenas no âmbito do Modelo Canônico do abnteX2
% ---
\usepackage{lipsum}				% para geração de dummy text
\usepackage[brazilian,hyperpageref]{backref}	 % Paginas com as citações na bibl
\usepackage[alf]{abntex2cite}	% Citações padrão ABNT
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{listings}
\hypersetup{
	colorlinks=false,
	pdfpagemode=FullScreen,
	pdftitle={benchmark bancos de dados multi arquitetura},
}
\hypersetup{final}
\urlstyle{same}
\renewcommand{\backrefpagesname}{}
% Texto padrão antes do número das páginas
\renewcommand{\backref}{}
\titulo{Benchmark de desempenho entre bancos de dados em diferentes arquiteturas}

\autor{Miguel Magalhães Lopes}
\local{Rio Pomba}
\data{2022}
\orientador{Msc. Gustavo Henrique da Rocha Reis}
%\coorientador{CICLANO}
%\instituicao{}
\tipotrabalho{Trabalho de Conclusão de Curso}

\preambulo{Trabalho de Conclusão apresentado ao Campus Rio Pomba, do Instituto Federal de Educação, Ciência e Tecnologia do Sudeste de Minas Gerais, como parte das exigências do curso de Bacharelado em Ciência da Computação para a obtenção do título de Bacharel em Ciência da Computação.}
\definecolor{blue}{RGB}{41,5,195}
\makeatother
\graphicspath{ {./imagens} }
% ---

% ---
% Espaçamentos entre linhas e parágrafos
% ---

% O tamanho do parágrafo é dado por:
\setlength{\parindent}{1.3cm}

% Controle do espaçamento entre um parágrafo e outro:
\setlength{\parskip}{0.2cm}  % tente também \onelineskip

% ---
% compila o indice
% ---
\makeindex
% ---

% ----
% Início do documento
% ----
\begin{document}
	\frenchspacing
	\begin{center}
		\textbf{
			INSTITUTO FEDERAL DE EDUCAÇÃO, CIÊNCIA E TECNOLOGIA DO SUDESTE DE MINAS GERAIS - CAMPUS RIO POMBA}
	\end{center}

	\imprimircapa
	\imprimirfolhaderosto*

	\begin{fichacatalografica}
		\vspace*{\fill}					% Posição vertical
		\hrule							% Linha horizontal
		\begin{center}					% Minipage Centralizado
			\begin{minipage}[c]{12.5cm}		% Largura

				\imprimirautor

				\hspace{0.5cm} \imprimirtitulo  / \imprimirautor. --
				\imprimirlocal, \imprimirdata-

				\hspace{0.5cm} \pageref{LastPage} p. : il. (algumas color.) ; 30 cm.\\

				\hspace{0.5cm} \imprimirorientadorRotulo~\imprimirorientador\\

				\hspace{0.5cm}
				\parbox[t]{\textwidth}{\imprimirtipotrabalho~--~Instituto Federal de Educação, Ciência e Tecnologia do Sudeste de Minas, Campus Rio Pomba,
					\imprimirdata.}\\

				\hspace{0.5cm}

				\hspace{8.75cm} %CDU 02:141:005.7\\

			\end{minipage}
		\end{center}
		\hrule
	\end{fichacatalografica}
	% ---

	% ---
	% Inserir errata
	% ---
	%\begin{errata}
	%Elemento opcional da \citeonline[4.2.1.2]{NBR14724:2011}. %Exemplo:

	%\vspace{\onelineskip}
	%
	%FERRIGNO, C. R. A. \textbf{Tratamento de neoplasias ósseas apendiculares com
		%reimplantação de enxerto ósseo autólogo autoclavado associado ao plasma
		%rico em plaquetas}: estudo crítico na cirurgia de preservação de membro em
	%cães. 2011. 128 f. Tese (Livre-Docência) - Faculdade de Medicina Veterinária e
	%Zootecnia, Universidade de São Paulo, São Paulo, 2011.

	%\begin{table}[htb]
	%\center
	%\footnotesize
	%\begin{tabular}{|p{1.4cm}|p{1cm}|p{3cm}|p{3cm}|}
	%  \hline
	%   \textbf{Folha} & \textbf{Linha}  & \textbf{Onde se lê} % & \textbf{Leia-se}  \\
	%    \hline
	%    1 & 10 & auto-conclavo & autoconclavo\\
	%   \hline
	%\end{tabular}
	%\end{table}
	%
	%\end{errata}
	% ---

	% ---
	% Inserir folha de aprovação
	% ---

	% Isto é um exemplo de Folha de aprovação, elemento obrigatório da NBR
	% 14724/2011 (seção 4.2.1.3). Você pode utilizar este modelo até a aprovação
	% do trabalho. Após isso, substitua todo o conteúdo deste arquivo por uma
	% imagem da página assinada pela banca com o comando abaixo:
	%
	% \includepdf{folhadeaprovacao_final.pdf}
	%
	\begin{folhadeaprovacao}

		\begin{center}
			{\ABNTEXchapterfont\large\imprimirautor}

			\vspace*{\fill}\vspace*{\fill}
			{\ABNTEXchapterfont\bfseries\Large\imprimirtitulo}
			\vspace*{\fill}

			\hspace{.45\textwidth}
			\begin{minipage}{.5\textwidth}
				\imprimirpreambulo
			\end{minipage}%
			\vspace*{\fill}
		\end{center}

		Trabalho aprovado. \imprimirlocal, 00 de maio de 2022.

		\assinatura{\textbf{\imprimirorientador}, Orientador, IF Sudeste MG - Rio Pomba}
%		\assinatura{\textbf{CICLANO}, Coorientador, IF Sudeste MG - Rio Pomba}
		\assinatura{\textbf{Msc. Bianca Portes de Castro} \\ IF Sudeste MG - Rio Pomba }
		\assinatura{\textbf{Msc. Sérgio Muinhos Barroso Lima} \\ IF Sudeste MG - Rio Pomba }
		%\assinatura{\textbf{Professor W} \\ IF Sudeste MG - Rio Pomba}

		\begin{center}
			\vspace*{0.5cm}
			{\large\imprimirlocal}
			\par
			{\large\imprimirdata}
			\vspace*{1cm}
		\end{center}

	\end{folhadeaprovacao}
	% ---


	% ---
	% Dedicatória
	% ---
	\begin{dedicatoria}
		\vspace*{\fill}
		\begin{flushright}
			Este trabalho é dedicado a todos\\
			aqueles que me inspiraram e me ajudaram durante essa caminhada, em especial\\
			minha família e amigos.
		\end{flushright}
	\end{dedicatoria}
	% ---

	% ---
	% Agradecimentos
	% ---
	\begin{agradecimentos}
    Agradeço a todos que me apoiaram durante todo o caminho deste curso e deste trabalho em especial meus colegas de curso e minha namorada.
	\end{agradecimentos}

	% resumo em português
	\begin{resumo}
		\noindent
    Este trabalho busca trazer um comparativo entre as capacidades reais de um servidor AMD64 e um servidor ARM em ambiente de banco de dados. Foi desenvolvida uma metodologia e um algoritmo para que a comparação pudesse ser a mais direta possível e mais simples de se aplicar uma escalabilidade de desempenho comparado. Foram utilizados os bancos de dados MariaDB e Postgres para simular os mais diversos tipos de operações básicas em bancos de dados como seleção, inserção e edição de dados.
    durante os testes foram utilizados Docker e limitações generalizadas no sistema operacional para que ambas as máquinas fossem o mais parecidas a nível de hardware, sendo assim podendo ser consideradas iguais apenas com diferença em suas arquiteturas, ao final dos testes os resultados foram tão próximos que apenas no consumo energético ficaram realmente evidentes.
		\vspace{\onelineskip}

		\noindent
		\textbf{Palavras-chaves:} ARM. AMD64. benchmark. servidor. MariaDB. Postgres. comparativo.
	\end{resumo}

	% resumo em inglês
	\begin{resumo}[Abstract]
		\begin{otherlanguage*}{english}
			\vspace{\onelineskip}
			\noindent
    This work seeks to bring a comparison between the real capabilities of an AMD64 server and an ARM server in a database environment. A methodology and an algorithm were developed so that the comparison could be as direct as possible and simpler to apply a scalability of compared performance. MariaDB and Postgres databases were used to simulate the most diverse types of basic operations in databases such as data selection, insertion and editing.
    during the tests, Docker and generalized limitations in the operating system were used so that both machines were the most similar in terms of hardware, thus being able to be considered the same only with difference in their architectures, at the end of the tests the results were so close that only in energy consumption were really evident.
			\vspace{\onelineskip}

			\noindent  \textbf{Key-words}:  ARM. AMD64. benchmark. server. MariaDB. Postgres. comparative.
		\end{otherlanguage*}
	\end{resumo}
	\urlstyle{same}

	\pdfbookmark[0]{\listfigurename}{lof}
	\listoffigures*
	\cleardoublepage

	\pdfbookmark[0]{\listtablename}{lot}
	\listoftables*
	\cleardoublepage

	\DeclareRobustCommand{\beginAutoTable}[4]{
		%nome da tabela e label
		%cabeçalho
		%quantidade total de colunas
		%formatação da tabela
		\label{tab:#1}
		\begin{longtable}{#4}
			\caption{#1}
			\\ \hline \multicolumn{#3}{c}{\textbf{#1}} \\ \hline
			#2 \\ \hline \endfirsthead
			#2 \\ \hline \endhead
		}
	\newenvironment{easyTableAuto}[4]{
		\beginAutoTable{#1}{#2}{#3}{#4}
		}{
		\end{longtable}
	}
	\newenvironment{easyTable2}[2]{
		\beginAutoTable{#1}{#2}{2}{p{.15\textwidth}|p{.80\textwidth}}
	}{
	\end{longtable}
}
\newenvironment{easyTable3}[2]{
	\beginAutoTable{#1}{#2}{3}{p{.16\textwidth}|p{.1\textwidth}|p{.70\textwidth}}
}{
\end{longtable}
}
\DeclareRobustCommand{\myref}[2]{
%\textit{#2}$^{\text{#1\ref{#1:#2}}}$
\textit{#2} \ref{#1:#2}
}
\newcounter{sig}
\DeclareRobustCommand{\sig}[1]{
\refstepcounter{sig}
\label{sig:#1}
\item[#1]
}
\newcounter{ch}
\DeclareRobustCommand{\ch}[1]{
\refstepcounter{ch}
\chapter{#1}
\label{ch:#1}
}
\newcounter{sec}
\DeclareRobustCommand{\sec}[1]{
\refstepcounter{sec}
\section{#1}
\label{sec:#1}
}
\newcounter{subsec}
\DeclareRobustCommand{\subsec}[1]{
\refstepcounter{subsec}
\subsection{#1}
\label{subsec:#1}
}
\newcounter{subsubsec}
\DeclareRobustCommand{\subsubsec}[1]{
\refstepcounter{subsubsec}
\subsubsection{#1}
\label{subsubsec:#1}
}
%\patchcmd{\verbatim@input}{\@verbatim}{\scriptsize\@verbatim}{}{}

\newcounter{anexo}
\DeclareRobustCommand{\anexo}[2]{
\refstepcounter{anexo}
\label{anexo:#1}
\lstinputlisting{{\detokenize{apendices/#2}}}
}

\newcounter{imagem}
\DeclareRobustCommand{\imagemsvg}[3]{
%lable
%arquivo
%caption
\refstepcounter{imagem}
\begin{figure}[htbp]
\includesvg[draft,width=\textwidth]{#2}
\caption{#3}
\label{imagem:#1}
\end{figure}
}
\DeclareRobustCommand{\imagempng}[3]{
%lable
%arquivo
%caption
\refstepcounter{imagem}
\begin{figure}[htbp]
\includegraphics[width=\textwidth]{#2}
\caption{#3}
\label{imagem:#1}
\end{figure}
}

\DeclareRobustCommand{\apendicesvg}[2]{
%lable
%arquivo
%caption
\refstepcounter{imagem}
\begin{figure}[htbp]
\includesvg[draft=true,width=\textwidth]{{\detokenize{#1}}}
\caption{#2}
\label{imagem:#1}
\end{figure}
}

\begin{siglas}
\sig{DACC} Departamento Acadêmico de Ciência da Computação

\sig{UFJF} Universidade Federal de Juiz de Fora

\sig{ARM} ARM, originalmente Acorn RISC Machine, e depois Advanced RISC Machine, é uma família de arquiteturas RISC desenvolvida pela empresa britânica ARM Holdings

\sig{x64} evolução da arquitetura \myref{sig}{x86} evoluida para 64 bit

\sig{AMD64} outra nomenclatura para \myref{sig}{x64}

\sig{x86} arquitetura 32 bit comum nos computadores domésticos atuais

\sig{aarch64} evolução 64bit da arquitetura \myref{sig}{ARM}

\sig{JVM} JVM (Java Virtual Machine) é uma máquina abstrata.É uma especificação que fornece um ambiente de tempo de execução no qual o bytecode do java pode ser executado.
As JVMs estão disponíveis para muitas plataformas de hardware e software (ou seja, a JVM depende da plataforma).

\sig{IOT} internet das coisas,

\sig{SBC} single board computer,ou computador de placa única,é um tipo de computador utilizado hoje em dia em que no mesmo chipset você encontra todas as estruturas principais do computador estão juntas obrigatoriamente os SBC possuem CPU e memória RAM e podem conter outras estruturas de um computador como a ponte norte e a ponte sul

\sig{SoC} System on a Chip,outra nomenclatura para \myref{sig}{SBC}

\sig{WINE} O Wine é uma camada de tradução capaz de executar aplicações Windows em Linux e em outros sistemas operacionais compatíveis com POSIX

\sig{die} circuito integrado composto de material semi condutor gravado a laser que contem as trilhas eletrônicas dentro de um processador de computador

\sig{TPU} tensor processing unit,é uma hardware dedicado para processamento de inteligencia artificial desenvolvido pela google que é um ASIC, application-specific integrated circuits, um cirquito integrado de aplicação especifica, assim como uma GPU é um ASIC para equações matemáticas de matriz

\end{siglas}


\tableofcontents*

\textual
\setcounter{page}{1}
% ---------------------------------------------------------------------------------------------
% Introdução
% ---------------------------------------------------------------------------------------------
\chapter*{Introdução}
\addcontentsline{toc}{chapter}{\textbf{Introdução}}
\markright{Introdução}
\label{ch:introducao}
Com a  crescente adoção de processadores com arquitetura \myref{sig}{ARM} que entregam eficiência energética superior a comumente utilizada nos computadores e servidores convencionais (arquitetura \myref{sig}{x86}), que também tem uma versão com arquitetura de 64 bits (\myref{sig}{x64}), se faz necessária uma avaliação sobre a possível substituição dos computadores existentes pelos com processador \myref{sig}{ARM}.
Esta pesquisa foi desenvolvida considerando o uso de servidores \myref{sig}{ARM} presentes no mundo corporativo, de tal forma que foi feita uma comparação na utilização deste tipo de processadores para a simulação de uma aplicação de banco de dados. Essa aplicação simula, de forma realística, a utilização de uma base de dados de uma locadora de filmes.

Para que pudessem ser comparadas as arquiteturas, foi utilizado um cenário de benchmark de desempenho entre elas. O benchmark é um processo em que a máquina tem uma alta carga de uso aplicada sinteticamente para que possam ser registrados os dados de uso de hardware. Desta forma é possível encontrar valores numéricos para fazer uso de comparação com outros resultados desse benchmark em outras máquinas. Para que um benchmark seja válido é necessário que o cenário de teste feito em uma arquitetura seja feita, de forma idêntica, na outra arquitetura. Neste caso foi selecionado o teste de operações de bancos de dados distintos deforma padronizada, já que os testes são feitos de forma padronizada, esses testes podem ser replicados em qualquer arquitetura de hardware e no final são comparáveis por conta disso.

Para que pudesse ser testados o uso de CPU, RAM e disco foi criado um programa para a geração de dados baseado na biblioteca Faker implementado na linguagem Python. Os dados são gerados para cada país, em idiomas e caracteres compatíveis com a região escolhida, o que permite que estes dados como, nome, telefone,endereço e até mesmo usuário e senha sejam compatíveis com a região escolhida. Essa forma de inserção foi escolhida, já que um preenchimento de dados totalmente aleatórios e de tamanho fixo, poderia apresentar discrepância em relação ao desempenho em um ambiente real de uso, afetando tanto o tempo, quanto a carga dos processadores de forma negativa. Os dados utilizados em cada ciclo de teste foram exatamente os mesmos, simulando o uso em ambiente real, desta forma o benchmark se torna aplicável à realidade e de mais fácil comparação de desempenho que outros mecanismos de benchmark encontrados.

Com os resultados dos testes de benchmark obtidos, foi esperado encontrar alguma prova se a arquitetura \myref{sig}{AMD64} é substituível pela arquitetura \myref{sig}{aarch64} em servidores, já que o maior custo de manutenção de servidores em empresas é o gasto energético, sendo que o gasto energético de um processador \myref{sig}{ARM} é muito menor.
Durante os próximos capítulos foram descritas:
\begin{itemize}
    \item Fundamentação teórica, capítulo 1: explica as tecnologias e programas utilizados durante a pesquisa;
    \item Trabalhos relacionados, capitulo 2: capítulo onde são referenciados e explicados trabalhos relacionados ao apresentado neste texto, mostrando pontos e ideias relevantes que os respectivos autores encontraram durante suas pesquisas;
    \item Testes, capitulo 3: neste capítulo são descritos os testes que foram feitos e o que cada um testou;
    \item Metodologia, capitulo 4: nesta parte do texto é apresentada a metodologia e é explicadas as bibliotecas criadas para o programa, assim como o próprio programa base e como os dados foram processados após o término dos testes;
    \item Resultados, capítulo 5: nesse capítulo são explicados os resultados obtidos em cada um dos testes que foram detalhados no capítulo 3, apresentando uma análise de seus resultados e conclusões obtidas em cada um deles, assim como os problemas que foram encontrados durante o decorrer dos testes e do desenvolvimento;
    \item Conclusão, capítulo 6: por fim nesse capítulo os resultados do capítulo 5 são analisados como um todo e é encontrado o resultado final do texto onde é explicado o que foi concluído com o benchmark feito.
\end{itemize}


\ch{Fundamentação teórica}

\sec{Virtualização}
Virtualização é uma técnica usada para simular um computador dentro de outro, dependendo do método utilizado ele pode simular apenas uma camada, ou mais de uma das camadas de hardware e software de um computador. Um sistema virtualizado pode receber vários nomes de acordo com o método utilizado para virtualizar, alguns dos mais utilizados atualmente são máquinas virtuais e containers Docker, onde a principal diferença deles é a camada de hardware na qual ele é executado.

\sec{Máquina virtual}
Uma máquina virtual é executada como um programa dentro de um sistema operacional, elas possuem várias limitações, dentre elas, um menor desempenho de processamento, além de não permitir acesso a outros hardwares do computador.
Majoritariamente neste tipo de virtualização é permitido o acesso direto à memória principal, aos dispositivos de armazenamento de dados e a hardwares plug’n’play como dispositivos de entrada e saída USB. 
Entretanto, não é possível utilizar placas de vídeo ou outros hardwares PCI de forma eficiente.


\sec{Arquiteturas}
Por definição, arquitetura de computador é um conjunto de circuitos eletrônicos padronizados associados a um conjunto de instruções, de forma a simplificar a programação para que executem comandos diferentes do binário. Os compiladores utilizam esse conjunto de instruções para que o código de uma linguagem de programação de alto nível seja convertido para uma de baixo nível. A arquitetura também define e limita várias propriedades do hardware, como quantidade máxima de RAM, de armazenamento, suporte de saída de vídeo, capacidades de rede, entre outros.

As arquiteturas também podem ser definidas além da CPU, o que inclui a GPU, \myref{sig}{TPU} e vários outros módulos de hardware de um computador. Inclusive existem arquiteturas especiais que são aplicadas em nível de software, que não são necessariamente arquiteturas de computador, mas sim um tipo diferente de arquitetura, 
que existem em máquinas abstratas e que simplificam a programação de uma linguagem para que ela funcione com maior compatibilidade em várias máquinas de arquiteturas e de hardware diferentes. 
Portanto, se faz necessário otimizações na parte do código e da máquina abstrata, como o caso da \myref{sig}{JVM} da linguagem Java.

No entanto, as arquiteturas não são limitadas apenas a esses exemplos previamente citados, elas também podem estar presentes em todos os tipos de circuitos integrados, como por exemplo os processadores de roteadores e aparelhos \myref{sig}{IOT}, tais como lâmpadas e tomadas inteligentes.
Isso quer dizer que uma arquitetura não necessariamente é algo que precise de um hardware potente ou que só funcione ou exista em computadores, mas sim a forma como os algoritmos são interpretados no hardware, sendo eles algoritmos como softwares de computador ou como sequências de instruções realizadas por alguém.
Sendo assim, desde que exista um software e um hardware que se comuniquem, existe uma arquitetura e provavelmente houve uma conversão da linguagem de programação para a linguagem de máquina dessa arquitetura neste dispositivo.

Após o lançamento dos MacBooks de 2020 com processador M1, que funcionam com a arquitetura \myref{sig}{aarch64}, a Apple lançou uma camada de compatibilidade dos softwares com arquitetura \myref{sig}{AMD64} para \myref{sig}{aarch64} chamado de Rosetta2. Esse software funciona parcialmente como um emulador, exceto que ele faz as adaptações num nível mais próximo da máquina real e do sistema operacional nativo da máquina, resultando num desempenho muito superior a qualquer emulador existente. O Rosetta2 funciona de forma análoga ao projeto \myref{sig}{WINE} do Linux que reinterpreta os programas do Windows para funcionarem no Linux. Em alguns casos essa conversão leva a uma pequena perda de desempenho por esse processo de reinterpretação, em outros essa perda é bem mais visível, e podendo até mesmo, em raros casos, ocorrer um ganho de desempenho em comparação com o sistema original.

Existem arquiteturas com propósitos diferentes, como o caso dos processadores \myref{sig}{ARM} que foram pensados para entregar uma grande eficiência energética, enquanto os processadores \myref{sig}{x86} foram pensados para apresentarem grande poder de processamento independentemente do consumo energético. Entretanto, o principal propósito da arquitetura \myref{sig}{ARM} não é se diferenciar tanto da arquitetura \myref{sig}{x86}, mas sim tornar os computadores mais energeticamente eficientes, um exemplo disso é que um computador doméstico comum utiliza de 200 a 300w por hora, enquanto um computador raspberry pi 4, que é o computador \myref{sig}{ARM} mais potente da marca mais popular dentre os \myref{sig}{SBC} atualmente, consome em torno de 15w por hora. É uma grande diferença, principalmente levando em consideração que ambos têm a possibilidade de rodar os mesmos programas open source de Linux, tanto editores de texto, navegadores de internet quanto IDEs de programação que estão disponíveis para ambos. Para um usuário médio não existe limitação ao se migrar de um para o outro.
\begin{easyTableAuto}{Tabela comparativa da filosofia de implementação RISC X CISC}{Comparativo & RISC & CISC}{3}{p{.3\textwidth}|p{.3\textwidth}|p{.3\textwidth}}
Funcionamento das instruções & instruções atômicas & instruções complexas\\ \hline
Ciclos de \textit{clock} necessários & uma instrução por ciclo & vários ciclos por instrução \\ \hline
Acesso a RAM & instruções dedicadas para isso & varias instruções fazem isso\\ \hline
Tamanho das instruções & tamanho fixo & vários tamanhos diferentes \\ \hline
Quantidades de instruções & somente o essencial para otimizar o uso de CPU & todas possíveis para simplificar a programação \\ \hline
Pipeline & existe desde o principio & implementado posteriormente \\ \hline
\end{easyTableAuto}

As operações RISC são o mais simples possível e diversos métodos de programação são necessários para que algumas operações sejam realizadas, isso apesar de tornar a execução mais otimizada, também faz com que a programação seja mais difícil.
As operações de um processador levam uma quantidade definida de pulsos elétricos gerados por um cristal oscilador para serem executadas, um ciclo equivale ao número de oscilações por segundo que o cristal consegue fazer.

Nos processadores RISC existem apenas duas instruções que conseguem acessar a RAM, todas as operações e instruções são lidas para o cache do processador e apenas utilizadas as que se encontram dentro dele, já que após uma operação ter sido encerrada o resultado dela é movido para a RAM. A memória RAM apenas é acessada antes da operação de um processador RISC ter sido terminada caso não haja espaço suficiente no cache do processador para a operação ser concluída, nesse caso os dados seriam alternados entre a RAM e o cache do CPU o mínimo de vezes possíveis. 

No entanto, essa forma de troca de dados entre o cache e a RAM não ocorre no CISC, onde várias operações podem ter essa movimentação de dados entre cache e RAM mesmo se ainda houver espaço no cache do CPU.
Os processadores RISC desde os seus primórdios buscaram utilizar pipelines para otimizar o tempo de uso de CPU, o que não pode ser dito dos processadores CISC.

\subsec{ARM}
A arquitetura \myref{sig}{ARM} é baseada na arquitetura RISC, uma arquitetura de processadores concorrente da CISC que veio a se tornar a \myref{sig}{AMD64}. A primeira revisão da arquitetura \myref{sig}{ARM}, chamada de ARM-1, foi lançada em 1985, no mesmo ano foi lançada a segunda revisão ARM-2, que apresentava mais funcionalidades e desempenho no mesmo tamanho de \myref{sig}{die}. Já em 1989 foi lançada a terceira revisão, chamada ARM3 operando a 25MHz.
De forma geral a arquitetura ARM sempre apresentou \myref{sig}{die} menor e mais eficiente em comparação com a arquitetura \myref{sig}{x86} e suas variantes. O principal foco de mercado das primeiras revisões da arquitetura ARM foram sistemas embarcados que não dependiam de tantas operações por segundo em comparação com computadores convencionais como servidores, computadores de mesa e notebooks.

Com o tempo a arquitetura ARM alterou o ARM3 até que chegou a arquitetura ARMv6 \cite{ARMv6Manual} onde ela começou a ficar comparativamente mais potente ao ponto de ser mais amplamente utilizada. Após essa revisão, lançada por volta de 2007, a arquitetura ARM começou a ser amplamente utilizada para computação com o advento dos smartfones como conhecemos hoje em dia \cite{histARMEmbarcados}.
A arquitetura ARM passou por mais 3 grandes revisões após o ARMv6, chamadas \cite{ARMhist}:
\begin{itemize}
 \item ARMv7: foi a arquitetura mais amplamente utilizadas em smartfones no principio do Android, iOS e Windows Phone. Isso devido aos seus ganhos em economia energética e processamento em relação ao espaço necessário para o processador funcionar corretamente em relação as outras alternativas existentes no mercado;
 \item ARMv8: foi a primeira implementação oficial das instruções 64bit, também conhecida como \myref{sig}{aarch64}, essa foi a revisão que possibilitou a criação dos tablets e notebooks com processador ARM que existem atualmente;
 \item ARMv9: é a revisão mais atual da arquitetura, nela foram implementadas várias partes dedicadas a propósitos específicos, como criptografia, virtualização e outras aplicações mais específicas como inteligência artificial. Essa revisão possui 3 RAMificações cada uma focada para um dos RAMos que a ARM trabalha, são eles: aplicações gerais, processadores e micro controladores; onde cada uma possui cortes ou ampliações para melhor se adequar ao propósito.
\end{itemize}

\sec{Bancos de dados}
Banco de dados é um método de armazenamento de dados de forma estruturada para que as informações sejam associadas e consultadas com mais rapidez. Os dados também podem ser armazenados de forma a economizar espaço de armazenamento e, dependendo da otimização do banco de dados, também dá a possibilidade de realizar redundâncias de segurança dos dados.

Sistemas de computadores, dos mais diversos tipos, utilizam banco de dados para armazenamento de suas informações. Dessa forma, as informações ficam estruturadas em um formato padrão, o que permite acesso rápido a elas. Os tipos de bancos de dados analisados são sistemas SQL relacional (os mais genéricos), de forma que podem ser utilizados no máximo de aplicações diferentes possíveis. Isso faz com que esses tipos de sistemas sejam os melhores para serem simulados.

A partir dessas características, justifica-se a escolha dos bancos de dados como meio do benchmark realizado para essa comparação de arquiteturas.

\sec{Docker}
Docker é um sistema de virtualização de máquinas que busca simplicidade no gerenciamento e entrega de desempenho do hardware real da máquina que está sendo usada. Este sistema executa as chamadas de seus ambientes virtualizados, nomeados de contêineres, no mesmo nível do sistema operacional, logo acima do \textit{Kernel} do sistema, isto possibilita dedicar parte do hardware da máquina real para um contêiner. Permite, dentre outras possibilidades, reservar de forma fixa porções do hardware, como por exemplo, definir limite de memória RAM ou quantidade de núcleos de CPU utilizados.

Outra funcionalidade do Docker é a implementação de volumes, que é a forma de isolar as pastas do contêiner, e mantê-las mesmo se houver a necessidade do contêiner ser reinstalado para realizar atualização.

O Docker foi desenvolvido a pedido da Google utilizando a linguagem GO, desenvolvida pela própria empresa, e foi criado visando a simplificação da gerência dos clusters de processamento da Google, que utilizam dezenas de computadores mais fracos para fazer o processamento. Portanto essa implementação tem um menor custo para compor um ambiente de processamento tão potente quanto em um ambiente com poucas máquinas de maior valor. O Docker também facilita a manutenção devido à fácil reimplementação de um contêiner caso a máquina tenha algum problema \cite{dockerdoc}.

\subsec{Contêiner Docker}
Um container Docker executa em um nível logo acima do \textit{Kernel}, isto permite que qualquer dispositivo, ou dado disponível na máquina real do usuário, possa ser acessível por um container Docker. Além disso, ele possui maior eficiência de uso de memória RAM e de CPU em comparação às máquinas virtuais ou outros métodos de virtualização.

Os containers Docker são imaginados para isolar programas do sistema operacional da máquina real, evitando que algum problema ocorra com o programa e afete a máquina real do usuário e em muitas situações são utilizados como sistemas de desenvolvimento virtualizados, facilitando a replicação e correção dos problemas durante a criação de um programa.

Além disso, os Containers também costumam ter implementação mais simples e rápida do que máquinas virtuais, visto que eles utilizam imagens Docker para sua criação, que são geradas a partir de arquivos Dockerfile ou a partir de um repositório.


\subsec{Dockerfile}
Os arquivos dockerfile são mais leves de transportar, mas demoram mais para serem instalados, uma imagem instalada dessa forma segue todos os parâmetros de criação de um script de instalação. Sendo assim, caso algum programa tenha sido especificado para ser compilado durante a instalação de  uma imagem, o mesmo será compilado todas as vezes que esta imagem for instalada. Ademais, os arquivos dockerfile também facilitam a modificação de uma imagem caso seja necessário.

\subsec{Imagem Docker}
Uma imagem Docker é gerada a partir de um arquivo dockerfile, e a principal diferença é que uma imagem Docker,  após feito o download, é extraída na máquina onde o contêiner Docker será executado. Uma imagem é composta de snapshots que correspondem às etapas de execução de uma dockerfile, ou seja, quando o download é realizado, apenas as partes diferentes de uma dockerfile previamente adquirida é baixada, o que facilita a atualização de contêineres.

As imagens Docker são distribuídas de duas formas: a partir de repositórios semelhantes aos dos sistemas Linux ou a partir de arquivos compactados, os repositórios são o método mais utilizado para se distribuir essas imagens, sendo que o principal repositório é o Dockerhub, o repositório oficial do Docker.



\sec{Docker-compose}
É um método de implementação de contêineres Docker que utiliza um arquivo de implementação para facilitar sua replicação, comumente utilizado junto de um arquivo de variáveis de ambiente .env. Esse arquivo contém variáveis para serem utilizadas na implementação do stack de contêineres, fazendo com que o mesmo arquivo docker-compose possa ser utilizado em mais de um ambiente ou em máquinas diferentes \cite{dockercomposedoc}.


\sec{Elasticsearch monitoring stack}
É um conhecido método open-source de monitoramento de logs, ele se baseia em três programas: Elasticsearch (indexador e processador de logs), Logstash (receptor e monitorador de logs) e Kibana (interface web para monitorar e trabalhar a exibição dos dados coletados), por isso é conhecido como ELK stack. Esses programas são baseados em Java e consomem uma quantidade considerável de memória RAM, motivo pelo qual o programa foi executado em uma máquina dedicada, para não influenciar na análise de desempenho das arquiteturas monitoradas.

O ELK stack foi utilizado sem qualquer um dos seus plugins de processamento de informações, fazendo uso apenas do Elasticsearch para filtrar os resultados encontrados, e gerar os gráficos exibidos neste trabalho a partir de uma aplicação que utiliza seus dados.

\sec{Portainer}
Portainer é um painel de administração do Docker, com o objetivo de simplificar a administração e configuração de várias opções mais complexas do Docker. Inclusive facilitando a definição de uso do hardware que será usado por cada contêiner, e facilitando o backup e download dos volumes e imagens criados, assim como a atualização dos mesmos para versões mais novas.

O Portainer permite controlar, a partir de diversos métodos, múltiplas máquinas executando docker com um controle unificado e simples. Além disso, a interface web permite também a execução de comandos Shell e a atualização dos contêineres de forma simplificada.

Outra característica interessante é a possibilidade de unificar a administração dos diversos servidores utilizados durante o desenvolvimento e testes, sendo necessário apenas instalar o Portainer em um deles, e nas outras máquinas apenas a ativação da conexão remota pelo Docker socket. Desta forma o monitoramento de logs, uso de hardware e outros podem ser feitos diretamente pelo navegador \cite{portainer}.


%\section{biblioteca faker}
%\label{sec:biblioteca faker}
\sec{Biblioteca Faker}
A biblioteca Faker é conhecida para a geração de \textit{mock data}, que são dados gerados aleatoriamente para testar a capacidade de um algoritmo em lidar com dados. Comumente utilizada na etapa de testes de um algoritmo, seja para testes de segurança, para se proteger de scripts automatizados de criação de contas ou algo semelhante, e também é utilizado para testes de estresse quanto para qualquer tipo de teste que dependa de dados reais.

Essa biblioteca foi selecionada, pois é de fácil utilização e compreensão, e possui documentação abrangente e suporte para múltiplos idiomas.
Ainda que a biblioteca suporte todos os tipos de geração mais completos na localização dos EUA, possui um tipo que corresponde a todo um perfil de usuário, contendo desde endereço até força de senha. Entretanto, para propósitos de testes, foram gerados apenas dados na localização do Brasil \cite{faker}.

%\section{biblioteca psutil}
%\label{sec:biblioteca psutil}
\sec{Biblioteca PSutil}
O PSutil (Python System and Process Utilities) é uma biblioteca multiplataforma para recuperação de informações na execução de processos, e utilização do sistema (CPU, memória, discos, redes, sensores). Essa biblioteca é muito útil para sistemas de monitoramento, limitação de recursos de processamento, perfil e gerenciamento de processos em execução, e implementa também funcionalidades oferecidas por linhas de comando e de sistemas baseados em UNIX tais como: ps, top, lsof, netstat, ifconfig, who, df, kill, free, dentre outros \cite{psutil}.

%\chapter{Trabalhos Relacionados}
%\label{ch: trabalhos relacionados}
\ch{Trabalhos Relacionados}

\sec{Um comparativo entre performance de bancos de dados MariaDB e MySQL com carga OLTP}%{A Comparison of Database Performance of MariaDB and MySQL with OLTP Workload}
Neste trabalho foram utilizados vários tipos de bancos de dados além do Postgres e do MariaDB para a comparação de performance, sendo o foco principal entre MariaDB e MySQL, além disso, foram monitorados CPU e memória RAM, e durante os testes foi usado um hypervisor chamado XEN nas máquinas com plataforma \myref{sig}{AMD64} e foram usados 4 GB de memória RAM nas máquinas dos testes. A mesma estrutura de BD foi utilizada em todos os servidores.

MariaDB e MySQL foram muito próximos em relação ao desempenho no primeiro teste, e no segundo teste MariaDB foi ligeiramente mais pesado em relação a CPU que MySQL \cite{MariaDBMySQLOLTP}.
% \imagempng{valores encontrados oltp  MariaDB}{artigos relacionados/oltp  MariaDB.png}{valores encontrados para o container  MariaDB para o 1 teste no artigo citado}
% \imagempng{valores encontrados oltp mysql}{artigos relacionados/oltp mysql.png}{valores encontrados para o container mysql para o 1 teste no artigo citado}
\newline\newline

\sec{Automação do Docker Swarm em SoCs ARM com suporte a MPI e análise de desempenho}%{Automatisation de Docker Swarm sur SoCs ARM avec support MPI et Analyse des Performances}
O artigo aborda uma análise de desempenho de Docker Swarm em \myref{sig}{SoC} \myref{sig}{ARM} onde foi usado MPI como base para os aplicativos. O sistema base dos contêineres é o Alpine Linux e foi utilizado em um ambiente onde uma ferramenta externa configura o ambiente MPI para processamento, Dessa forma foi possível automatizar a implementação e monitoramento do Docker Swarm.

Uma das ferramentas utilizadas é o processo de escaneamento das conexões ativas, usando o netstat para simular o monitoramento do Swarm de forma externa ao docker, e o método usado para experimentar a plataforma virtualizada foi o WRF, que é um método de previsão do tempo.

Para o ambiente de testes foram utilizados mini computadores Raspberry Pi versões 2 e 3, Nano Pi Neo, NTCP chip e Banana Pi, onde o ganho de desempenho de execução entre múltiplos núcleos é maior que o ganho de desempenho de múltiplas máquinas, segundo os autores do artigo. A porcentagem de ganho entre usar todos os núcleos do Raspberry é de 71\%, mas ao usar processamento dentro do swarm, a aceleração é de 17\% de 1 a 2 máquinas e de 2\% de 2 a 3 máquinas. De acordo com o autor, isso ocorre devido a baixa velocidade de comunicação entre os Raspberry Pi 3, ou seja, devido ao próprio \myref{sig}{SoC} dele.

Levando em conta o custo, para essa aplicação o uso de \myref{sig}{SoC} é viável, já que o WRF consegue apresentar resultados básicos diariamente ou até mesmo em intervalos de uma hora, de acordo com os resultados apresentados, o desempenho multi-core de apenas uma máquina é quase o mesmo que o de duas máquinas com multi-core em paralelo \cite{DockerSwarmsocmpi}.


\sec{Um estudo comparativo dos efeitos da paralelização em plataformas baseadas em ARM e Intel}%{A comparative study of the effects of parallelization  on arm and intel based platforms}
Este artigo utiliza a mesma proposta abordada nesta pesquisa, cada vez mais máquinas \myref{sig}{ARM} são utilizadas ao redor do mundo, tendo em vista que hoje em dia todos celulares utilizam processadores \myref{sig}{ARM} e os computadores estão começando a adotar essa arquitetura. O artigo aborda os múltiplos métodos de paralelização que são utilizados pelo autor  para realizar os testes, assim como as diferenças das implementações entre Intel e \myref{sig}{ARM}, já que no processador \myref{sig}{ARM} é impossível usar a GPU para os testes, devido ao sistema operacional utilizado não possuir drivers que possibilitasse esse uso.

Como métricas de monitoramento utiliza-se o total de frames por segundo (fps), visto que os algoritmos utilizados são algoritmos de processamento de imagem e potência em joule (J) utilizada pelas máquinas analisadas. O monitoramento de fps era feito por um monitor interno ao CPU e não um sistema externo, e várias das aplicações foram otimizadas para que funcionassem da melhor forma possível em cada CPU, de forma que na otimização interferisse o mínimo possível nos resultados dos testes. Essas otimizações foram feitas em forma de \textit{Kernel}s compilados diferentemente para o algoritmo.

A partir das métricas coletadas, o autor calculou aproximadamente o total de energia gasta para processar cada frame, ou seja, o total de energia gasto pela função do código no método serial que é o único exatamente igual em ambos. Existe uma diferença sutil, mas com maior eficiência energética por parte do processador \myref{sig}{ARM}, visto que após ser levado em conta o consumo e a quantidade de frames gerados, é aplicada uma fórmula para comprovar a eficiência energética dos dispositivos utilizados.
ao ser usado um dos vários \textit{Kernel}s diferentes, existe uma velocidade de processamento exponencialmente maior por parte da Intel que por parte do \myref{sig}{ARM} no melhor cenário, no pior, a velocidade por parte da Intel é 10 vezes maior que do \myref{sig}{ARM}.

O resultado é que nos melhores casos de cada \textit{Kernel}, o fps/J é maior no Intel que no \myref{sig}{ARM}. Mas essa diferença é sutil em um dos \textit{Kernel}s e alta em outro, onde no ultimo teste, que foi o pior teste de cada plataforma, foi observada uma eficiência em torno de 8.5 vezes maior no \myref{sig}{AMD64} em relação ao \myref{sig}{ARM}. Portanto concluímos que para um dos testes, teste SRAD, o ultrabook \myref{sig}{AMD64} utilizado é claramente superior, nos outros testes sua superioridade nos valores de fps não eram tão evidentes.
Entretanto o consumo energético do \myref{sig}{ARM} é 20\% menor no pior cenário, e no melhor é 60\% menor que o do \myref{sig}{AMD64} e seu consumo médio é de 50\% do consumo do \myref{sig}{AMD64}\cite{armvsintel}.

Como conclusão sobre os valores apresentados, foi possível comprovar que 8 anos depois, as placas \myref{sig}{ARM} existentes consomem o mesmo tanto de energia, mas possuem muito mais poder computacional. Enquanto que os computadores \myref{sig}{AMD64} portáteis, como o ultrabook utilizado, apesar de também consumirem uma quantidade equivalente de energia, não teve um ganho de performance tão grande quanto os computadores \myref{sig}{ARM}.


\sec{KVM, Xen e Docker: uma análise de desempenho para NFV baseado em ARM e computação em nuvem}%{KVM, Xen and Docker: a performance analysis for ARM based NFV and Cloud computing }
Trata-se de um estudo comparativo, onde os testes de desempenho são feitos com softwares  de benchmark que simulam o uso de servidor, estes benchmarks sintéticos foram usados para encontrar um sistema de virtualização com melhor performance para a virtualização de funções de rede para a arquitetura \myref{sig}{ARM}. Neste estudo foram utilizados KVM, Xen e Docker, estes métodos de virtualização conceituados nos servidores comerciais foram selecionados com o propósito de provar qual deles funciona melhor para entregar maior poder bruto de performance, enquanto mantém os aspectos de segurança necessários de um ambiente de mundo real.

Este estudo aborda as diferenças entre containers e hypervisors que se baseiam principalmente no fato de que os  containers utilizarem os recursos diretamente do \textit{Kernel} da máquina física, enquanto os hypervisors buscam virtualizar tudo e simular um hardware em cada aplicação virtualizada. No artigo são abordadas as vantagens para cada método de virtualização, as principais do container são que nele o acesso ao poder computacional bruto é maior e  nos hypervisors as camadas de segurança são maiores.

Dentre os dados encontrados, vale ressaltar que o Docker apresentou em média um resultado inferior quando comparado a velocidade de leitura e escrita em relação aos outros métodos de virtualização, apesar dessa diferença ser bem pequena numa aplicação de mundo real, onde a diferença não chega a 100 bps na maioria dos casos.

Outro ponto a se destacar é que na velocidade de transferência de rede o Docker apresentou, durante os testes,  valores mais lentos quando utilizados pacotes de poucos bytes, sendo que para pacotes de até 4 bytes os valores eram em média iguais, mas para pacotes de 8 a 32 bytes as diferenças de velocidade eram consideravelmente maiores, nos quais o Docker chega a ser cerca de 20 mbps mais lento que os outros métodos. Para pacotes maiores que isso, todos os métodos utilizam ao máximo a capacidade de transferência de rede do hardware utilizado (100 mbps).

Em conclusão, a autora disse que o KVM é mais simples de se portar para o \myref{sig}{ARM} em comparação ao Xen, já que o QEMU, no qual o KVM se baseia, existe em todas as arquiteturas e o Docker também já existe nativamente para o \myref{sig}{ARM}. As leituras de disco e rede funcionam melhor nos hypervisors devido aos métodos de cache que ele possui e o Docker não, outra informação que foi possível perceber também é que para pacotes grandes, as velocidades de todos os métodos comparados são iguais ou maiores que a implementação nativa em todos os testes de performance. A diferença entre eles é bem pequena, e varia de acordo com o teste, sendo assim o ideal seria uma solução mista onde seriam utilizado um método de virtualização dentro do outro para melhorar a escalabilidade e segurança, além de outros fatores dos ambientes reais\cite{KVMXenDocker}.

\sec{O Raspberry Pi: uma plataforma para benchmarks de desempenho replicáveis?}%{The Raspberry Pi: A Platform for Replicable Performance Benchmarks?}
As vantagens do uso do Raspberry Pi para benchmark, incluem sua facilidade de replicação usando SSH e Raspbian, e mostra as diferenças de resultados entre os diferentes Raspberry Pis usados nos testes. Os dados mostram que os resultados são bem aproximados, porém não são os mesmos.

Os Raspberry Pi usados foram o modelo 3, eles foram comprados do mesmo vendedor com um intervalo de duas semanas, para se provar que o dispositivo poderia ser de fácil replicação. Adicionalmente, foi comprado um terceiro Raspberry Pi de outro vendedor com um intervalo de diferença de alguns meses entre este e os dois anteriores, para comprovar que lotes de fabricação diferentes não influenciam a replicabilidade do dispositivo. Foi utilizada uma imagem de instalação padronizada, modificada pelos autores, usando como base o sistema operacional oficial da época: o Raspbian, isso foi feito para manter uma diferença zero em relação aos softwares usados, e a única diferença existente sendo apenas no hardware usado. Os cartões de memória utilizados nos computadores, também possuíam o mesmo tamanho mas não foram dados maiores detalhes sobre eles.

Um dos testes demandava bastante uso de espaço de disco, sendo assim, foi necessário adicionar mais memória externa por meio de um HD externo.
O autor chegou à conclusão de que o Raspberry Pi é uma boa plataforma para replicabilidade, principalmente se utilizado o Docker para padronizar o ambiente de replicação, além disso, também foi concluído que versões posteriores do Raspberry Pi poderiam melhorar ainda mais esse cenário devido às melhorias de hardware que ocorreriam
\cite{rpiplatformreplicable}.

Cerca de 4 anos após a conclusão deste trabalho, foi lançada uma versão nova e exponencialmente mais útil em termos de testes replicáveis, pois não há limitação às placas de 1gb de RAM e nem a limitação das baixas velocidades de transferência, tanto de disco quanto de rede. A partir disso, as conclusões do autor se comprovaram pouco tempo antes dessa versão ser lançada, e a tendência é de que continue lançando versões superiores, visto que após lançarem versões com 2gb e 4gb de RAM a Raspberry Pi Foundation lançou mais uma versão de 8gb de RAM e posteriormente uma versão estável do seu sistema operacional oficial, agora chamado de Raspberry Pi OS, com suporte a instruções 64 bits. Essas melhorias ocorreram apenas em uma nova revisão da placa, as futuras evoluções tendem a ser ainda maiores levando em conta as evoluções que têm ocorrido nas revisões da arquitetura \myref{sig}{ARM}.

\sec{HS06 Benchmark para um servidor ARM}%{HS06 Benchmark for an ARM Server}
Este artigo se resume em um benchmark e a análise de um servidor de arquitetura \myref{sig}{ARM}, esse servidor possui 12 slots para encaixes das placas de \myref{sig}{SoC}, cada placa de \myref{sig}{SoC} possui 4 CPUs com 4 núcleos, um pente de RAM, 4 portas SATA e um conector de porta 10GBE para cada CPU, resultando em 16 núcleos, 4 pentes e 16 portas SATA por placa \myref{sig}{SoC}. Cada servidor consome cerca de 300w de energia, entretanto apenas um \myref{sig}{SoC} foi utilizado para os testes.

Foram utilizados para essa comparação mais 5 servidores \myref{sig}{AMD64}, sendo 1 HP, 3 IBM e 1 Dell, o consumo energético do servidor \myref{sig}{ARM} entre os analisados é o menor e o mais potente em relação ao consumo energético, visto que de acordo com a tabela de resultados apresentada no texto o único que conseguiu mais que 1 ponto por watt gasto foi o computador \myref{sig}{ARM} \cite{hs06}.

Importante ressaltar que os resultados encontrados na máquina \myref{sig}{ARM} são inferiores em pontuação quando comparado com todos os outros servidores utilizados, mas ao mesmo tempo o consumo energético desse processador é exponencialmente menor que os outros servidores, principalmente ao se levar em conta que ele consome cerca de 5w de energia, enquanto os outros consomem entre 150w e 600w.

%\chapter{testes}
%\label{ch: testes}
\ch{Testes}
Os testes foram realizados utilizando duas máquinas, sendo uma delas um \myref{sig}{SBC} \myref{sig}{ARM} e a outra um notebook \myref{sig}{AMD64}, o \myref{sig}{SBC} utilizado foi um Orange Pi PC + \cite{opipc} e o notebook utilizado foi um Lenovo G405 \cite{G405}. O Orange Pi é baseado no processador Allwinner H3 \cite{h3}, além disso, nesta placa existem 3 portas USB 2.0, 1GB de memória RAM DDR3, uma porta de rede 10/100 e WI-FI 4. Essa configuração é relativamente datada e seu processador é um quad-core de 1.3GHz no seu \textit{clock} máximo. Ele ainda possui desempenho razoável caso seja considerado um uso básico, entretanto considerando todas suas especificações técnicas, este \myref{sig}{SBC} não é bom o bastante para substituir um computador atual satisfatoriamente, isso devido a sua limitação de memória RAM e da sua capacidade gráfica limitada.

O Lenovo G405 \cite{G405}, é um notebook com um processador AMD E1-2100 que é um dual-core com \textit{clock} máximo de 1.0GHz \cite{E1}, 2 GB de memória RAM DDR3, 1 portas USB 2.0, 2 portas USB 3.0, porta de rede Gigabit e 2 portas SATA3. Porém, as portas SATA e as USB 3.0 não foram usadas, já que o propósito é manter as duas máquinas o mais próximas possível em relação as velocidades e capacidades de hardware, para que houvesse proximidade em velocidade e especificações. Este computador também possui especificações técnicas obsoletas para os dias atuais, mas ainda é utilizável como um servidor doméstico.

Considerando as velocidades de armazenamento disponíveis para as máquinas utilizadas, o Orange Pi teve o seu SO armazenado na sua memória interna eMMC e no computador \myref{sig}{AMD64}, em um HD 2.5“ SATA3 de 500GB. A memória onde o sistema operacional do Orange Pi está armazenada apesar de ser bem mais lenta que um HD SATA3, já que a velocidade desta memória de armazenamento afeta apenas o tempo de inicialização do sistema operacional da máquina host e não dos containers, por isso foi considerado como equivalente. Para o armazenamento do Docker e seus dados nas duas máquinas foi utilizado um pendrive SanDisk Cruzer Blade 2.0 de 16GB, ambos comprados simultaneamente do mesmo vendedor. O Docker foi movido para eles utilizando as configurações do arquivo "daemon.json" do Docker que  define vários dados cruciais do funcionamento do processo daemon.

Alguns métodos que foram utilizados para manter as máquinas com equivalência foram limitar o \textit{clock} de ambos para que se mantivessem o mais próximo possível, modificar a quantidade de núcleos acessíveis pelos containers e definir a quantidade máxima de memória acessível pelos containers. No Orange Pi foi definido um \textit{clock} máximo de 1GHz, esse sendo um valor para facilitar os cálculos de equivalência, já que é o mesmo \textit{clock} máximo do notebook, sendo assim é uma comparação direta e não uma comparação em escala. O número de núcleos e a memória usada foram limitadas no stack do docker, a memória a 1024 MB, pois é a memória máxima do Orange Pi, e os núcleos para dois já que é o total do notebook.

Além disso, os softwares utilizados nessas máquinas foram versões do sistema Debian, no Orange pi foi o Armbian e no notebook o próprio Debian netinstall, ambos na versão Buster. A versão do Docker usado em ambos é a versão community edition 20.10.12 e a versão do docker-compose é a versão 2.2.3, sendo essas as versões mais atuais no início da execução dos testes. Todos os outros softwares instalados diretamente na máquina são irrelevantes para o propósito dos testes realizados, entretanto para evitar maiores prolemas de compatibilidade devido as variações do sistema operacional foram desativados vários serviços do systemd para que eles não afetassem na execução, os demais softwares relacionados aos testes serão descritos em \myref{sec}{Contêineres docker}.

A baixo estão descritos os testes utilizados para verificar a funcionalidade do código,o primeiro foi criado para que pudesse ser verificado manualmente se os dados gerados eram válidos para o contexto em ambas as máquinas e o segundo verificar para verificar a eficácia do algorítimo já funcional a nivel de comparação de poder de processamento das arquiteturas para essa geração de dados, primariamente para saber qual delas seria mais eficiente de se deixar gerando os dados enquanto outras etapas do desenvolvimento eram feitas.

\begin{itemize}
\item Um teste generalizado de funcionalidade com valores pequenos, onde  são testados uma quantidade X de elementos gerados e isso se repete um número Y de vezes para cada valor com o objetivo de saber uma média de tempo;
\item Um segundo teste para comprovar a eficácia com valores variáveis onde são gerados N ciclos de testes, sendo cada um deles com uma quantidade X+I de dados, onde X é a quantidade de dados do ciclo anterior e I um valor aditivo constante. Isso para saber se existe um tempo constante por elemento gerado ou se existe um tempo fixo mais um valor constante por elementos. Também são repetidas Y vezes dentro de cada ciclo para ter um valor de média de tempo e não apenas um valor único.
\end{itemize}


%\section{testes de tempo}
%\label{sec:testes de tempo}
\sec{Testes de tempo}
Os algoritmos principais do código são a geração do SQLite e a inserção nos BDs finais, sendo assim, ambos foram testados de formas diferentes e por métodos diferentes que serão descritos a seguir.

O algoritmo de geração de banco de dados é muito impactado pela quantidade de núcleos de CPU da máquina na qual foi rodada, mais do que a arquitetura ou \textit{clock} dela. Enquanto na inserção do banco de dados, as especificações do computador não pareceram interferir tanto quanto a aleatoriedade da consistência do funcionamento das \textit{threads}. Essa inconsistência acabou fazendo com que os testes tivessem que ser realizados sob supervisão, sendo muitas vezes necessário interromper uma etapa e depois reiniciá-la, além disso, várias das operações foram testadas de formas diferentes. Foram utilizadas sempre duas etapas de testes, sendo que pequenas modificações foram feitas em cada uma de acordo com o objetivo .

%\subsection{Teste de tempo de criação do BD e funcionalidade}
%\label{subsec:Teste de tempo de criação do BD e funcionalidade}
\subsec{Teste de tempo de criação do BD e funcionalidade}
O teste dessa etapa foi feito para, dentre outros objetivos, comprovar o funcionamento do algoritmo de geração de dados descrito em \myref{subsec}{geradorDeSql}. À medida que o desenvolvimento dessa biblioteca acontecia, testes de geração tiveram que ser feitos. Esses testes foram usados para saber a eficiência da geração de dados, isso porque os dados gerados levavam uma quantidade parcialmente aleatória de tempo para serem gerados, muito disso devido aos dados gerados serem semi-procedurais, descrito em \myref{subsec}{SQLite}. %semi-procedurais pois não segue corretamente a seed de geração procedural
Além disso, esse teste foi necessário devido a etapa de geração de dados de inserção \myref{sec}{Geração de dados}.

Na etapa principal desse teste, foi feito um teste aditivo, ademais foi feito um teste de 4 sub ciclos internos para quantidades diferentes de dados. Os testes desta etapa foram realizados em um PC \myref{sig}{aarch64} e dois PCS \myref{sig}{AMD64}, os dados resultaram em valores de tempo consistentes em relação a diferença de frequências dos computadores.

Sendo assim, se existe um valor de perda entre as arquiteturas para esse teste é um valor irrisório, pois o PC \myref{sig}{AMD64} mais potente apresentou testes cerca de 2 vezes mais rápidos e seu \textit{clock} é aproximadamente o dobro do \myref{sig}{aarch64}. Esses valores foram dados em relação ao tempo gasto por elemento em cada iteração. Com base nos resultados desses testes, desde que o computador tenha vários núcleos, o algoritmo será tão eficiente quanto, isso se deve ao fato de que levando em conta o PC com \myref{sig}{AMD64} com 4 núcleos e 4 \textit{threads} com um \textit{clock} que varia entre 3.6GHz e 4.0GHz, e um outro PC \myref{sig}{AMD64} de 2 núcleos e 4 \textit{threads} com um \textit{clock} que varia de 2.7GHz a 3.5GHz, os valores de tempo do PC \myref{sig}{AMD64} mais fraco levou cerca de 97\% do tempo do \myref{sig}{AMD64} mais potente, essa diferença aparentemente ocorre devido a duas tecnologias desses processadores:\newline
\begin{itemize}
\item Tecnologia Intel® Turbo Boost frequency 2.0: quando detecta que o computador precisa de mais potência aumenta o \textit{clock} do CPU;
\item Precision Boost Overdrive: essa tecnologia permite que o processador varie seu \textit{clock} de acordo com a necessidade, tanto aumentando quanto diminuindo para economizar energia.
\end{itemize}

Dessa forma, o \textit{clock} do processador foi diminuído, já que apenas um dos núcleos do processador estava em carga alta e essa tecnologia aparentemente leva em conta a porcentagem de todos os núcleos somados para funcionar. Essa informação se baseia nos métodos de economia de energia existentes nos notebooks \myref{sig}{AMD64} da mesma geração, visto que a \myref{sig}{AMD64} não detalha muito esse processo da sua tecnologia. Além disso, também foi realizado um segundo teste referente ao descrito acima, seguindo os mesmos parâmetros, mas utilizando o processamento paralelo.

O maior diferencial entre os testes da arquitetura \myref{sig}{ARM} e \myref{sig}{x86} foi observado que na arquitetura \myref{sig}{ARM}, devido a limitações dos recursos disponíveis, foi necessário utilizar um HD ao invés de um SSD. Em \myref{sec}{Geração de dados} é abordado o quanto isso impacta negativamente na velocidade da geração dos dados, resultando em valores piores que o teste anterior devido aos múltiplos acessos simultâneos do armazenamento.

\subsec{Teste de tempo de operação de inserção no BD}
Este teste foi proposto para ser um teste de dois tipos:
\begin{itemize}
\item O primeiro tipo de teste é um teste de tempo para operações sem erros acontecerem, onde os dados de inserção foram projetados para não apresentarem erro e sempre retornarem o resultado de operação inserida com sucesso, ou seja, caso não tenha ocorrido um problema do algoritmo de geração, serão geradas apenas operações que retornam uma mensagem dizendo que nenhum erro foi encontrado durante a inserção;

\item O segundo tipo de teste consiste na medição do tempo gasto, velocidades de leitura e de escrita de disco em aplicações com muitas inserções, já que muitos bancos de dados são usados primariamente para inserção de dados, como bancos de dados de log e de análise de dados. Um exemplo é o próprio \myref{sec}{Elasticsearch monitoring stack} que foi utilizado durante os testes, onde juntando a quantidade de operações de busca, listagem e filtragem não refletem a maioria das operações.
\end{itemize}

Para esse propósito foi utilizada uma variação do \myref{subsec}{Teste de tempo de criação do BD e funcionalidade}, onde ao invés de utilizar a inserção completa dos dados foi usada uma inserção fracionada, o que quer dizer que foram realizadas as operações em grupos de 5000 até completar os 100.000. Além disso, para que a máquina onde os testes finais foram executados não travasse foi definido um valor mais baixo de subprocessos, sendo um para cada núcleo do processador da máquina, onde três executavam os testes para o \myref{sig}{ARM} e três para o \myref{sig}{AMD64}.


\subsec{Teste de tempo de operações variadas no BD}
Após a confirmação da funcionalidade de forma satisfatória da geração do SQLite, foi iniciado o processo de interação com o banco de dados, sendo que foi dividido em algumas etapas. Primeiramente foi feita a verificação para confirmar o funcionamento correto da operação, essa etapa foi feita seguindo os testes de funcionalidade já descritos, onde uma pequena quantidade de operações era gerada usando a geração de SQLite em ambos os bancos de dados ao mesmo tempo. Além disso, foram feitos vários testes diferentes de acesso e manipulação do banco de dados , neles comprovou-se que as operações sem retorno e comunicação geral com o banco de dados foram os tiveram resultados satisfatórios, sendo assim, tornando-se possíveis as funcionalidades utilizadas da biblioteca \myref{subsec}{gerenciadorDeBD}.

Logo após a etapa de confirmação de funcionalidade não foram feitos testes de desempenho, já que eles foram conduzidos na etapa de testes paralelos, a qual se mostrou mais importante, visto que após a confirmação de funcionalidade não foram necessários mais testes para essa etapa.
Os dados foram gerados seguindo o seguinte processo:
50.000 operações de inserção foram geradas e posteriormente foi pedida ao programa a geração de 5.000.000 de operações randômicas, o que resultou em 50.000 operações de inserção seguidas de 4.950.000 operações randômicas geradas. Tendo em vista as grandes quantidades de dados gerados neste teste, para que a máquina onde os testes rodaram não travasse, foi utilizado o mesmo procedimento do \myref{subsec}{Teste de tempo de operação de inserção no BD}.

O principal propósito deste teste foi mostrar que muitos bancos de dados, principalmente de sites de comércio eletrônico e sistemas de gerência de estoque utilizam muitos tipos variados de operações, como as que foram utilizadas nesse teste. Sendo assim faz com que os resultados sejam facilmente aplicáveis para estas situações do mundo real.

\subsec{Teste de tempo de operações paralelas}
Essa foi a etapa que demandou maior quantidade de testes, estudos e desenvolvimento dos testes implementados, visto que além da programação paralela ser mais complexa que a programação linear, ela é bem mais difícil de ser testada. No entanto, apenas foi possível após as outras classes principais,\myref{subsec}{gerenciadorDeBD} e \myref{subsec}{geradorDeSql}, serem finalizadas.

Resumidamente, para a execução desse teste foi feita uma variação dos testes descritos em \myref{subsec}{Teste de tempo de operações variadas no BD}, porém com algumas modificações no código para que ele funcionasse de forma paralela.

\sec{Testes de eficiência}
Esses testes não levam apenas em conta o tempo que foi gasto para concluir a operação, mas também a forma como elas foram concluídas. Isso quer dizer que o tempo é um dos parâmetros para a sua avaliação, mas também a quantidade de falhas apresentadas, quantidade de CPU utilizada, e quantidade de interação do operador do algoritmo para que ele funcione como deveria.

%\subsection{teste de eficiencia de operações paralelas}
%\label{subsec:teste de eficiencia de operações paralelas}
\subsec{Teste de eficiência de operações paralelas}
Este foi o teste que mais demandou tempo e esforço dentre os realizados, pois devido a complexidade da programação paralela, como já dito em \myref{subsec}{Teste de tempo de operações paralelas}, uma quantidade maior de testes foram demandados, onde um dos grandes motivadores deste teste foi a etapa descrita em \myref{subsec}{Teste de tempo de operação de inserção no BD}.

Apesar desta etapa não ter sido complicada por si só, essas operações demandaram muitos testes manuais alternando o método de programação paralela, já que apresentava muitos erros e por isso ela demandou muita atenção. Para tentar solucionar esses erros foram usadas varias formas diferentes de se implementar a programação paralela, usando \textit{threads} ou subprocessos, quantidade de operadores paralelos, quantidade de elementos inseridos e forma dos elementos inseridos. Isso ocorreu pois em vários momentos foram apresentados erros randomicamente sem nenhum indício de justificativa.

Os erros apareciam em certos momentos quando era usado o processamento paralelo em \textit{threads} e em outros utilizando o processamento paralelo de subprocessos. Os mesmos erros eram corrigidos para que em sequência ao se implementar a próxima funcionalidade necessária o mesmo erro se repetisse. Isso gerou um ciclo no processo de desenvolvimento, onde muitos testes eram realizados e alternados com leitura de documentação das bibliotecas utilizadas, ou mesmo leitura de exemplos de códigos de fóruns e sites.

Essa etapa se misturou muito com o desenvolvimento do programa, pois a otimização do código foi necessária para que certas partes dele funcionassem corretamente, e também se misturou com as outras etapas de teste, já que as outras etapas acabaram por servir como etapas desse mesmo teste.

Durante estes testes foram detectados outros fatores interessantes, primeiramente que o Postgres apresenta uma demora maior na sua execução em comparação com o  MariaDB caso ocorram operações que retornem erro. Esse tipo de operação constitui a maioria das operações geradas para os testes, isso porquê a única que é garantida de não dar erro é a operação de inserção. Além disso, foi detectado que o quadro se inverte nas operações válidas, onde o Postgres é mais rápido que o MariaDB. Outro fator detectado é que caso seja feita uma implementação sequencial, a inserção dos dados será feita mais rapidamente do que em uma implementação paralela, além de apresentar uma quantidade de erros próximos a zero.

%\chapter{Metodologia}
%\label{ch: materiais e métodos}
\ch{Metodologia}
O desenvolvimento do software foi feito utilizando vários métodos de análise de log com o objetivo de agilizar a depuração, possibilitando que os dados gerados pudessem ser facilmente conferidos durante o desenvolvimento.
Durante esse capítulo, serão descritos os procedimentos mais importantes das etapas de desenvolvimento e funcionamento do software com o intuito de proporcionar um entendimento simples.

%\section{bibliotecas criadas}
%\label{sec:bibliotecas criadas}
\sec{Bibliotecas criadas}
Algumas bibliotecas foram criadas para que os testes fossem viáveis da forma desejada, algumas serão descritas mais detalhadamente, mas outras apenas serão citadas aqui.
Uma dessas bibliotecas foi para o tratamento de erros, essa classe é uma derivação da classe padrão do Python de exceção, que é composta de 3 classes, cada uma para um tipo de mensagem e tratamento de erro, com a função de simplificar e facilitar o debug do código durante a manipulação dele. Ela não faz real diferença em relação ao funcionamento em si, exceto no fato de poderem ser executadas correções específicas dos erros, quando um \textit{try-catch} é usado em uma função.

Além disso foi criada uma biblioteca de timer, usada nos testes de tempo do projeto, ela é uma contração de forma simplificada e com tratamento de erro do uso da biblioteca time do Python, essa técnica de medida de tempo é amplamente utilizada, mas apenas para fins de simplificação no momento do uso foi criada essa biblioteca.

%\subsection{loggingSystem}
%\label{subsec:loggingSystem}
\subsec{loggingSystem}
Essa biblioteca é a responsável por toda a gerência de logs e por todo o \myref{subsec}{monitorContainer}. Ela é basicamente um complemento da biblioteca padrão de logs do Python \cite{logging}, que tem uma implementação de comunicação com os servidores Logstash. A biblioteca de forma inteligente verifica se os parâmetros de comunicação com o Logstash estão funcionais, caso não estejam, automaticamente salva todos os logs em um arquivo com extensão "log" cujo nome foi informado durante a instância do objeto da classe.
A classe apenas consegue fazer essa verificação durante sua instância, caso após isso a conexão seja perdida os logs não são enviados a lugar algum.

As mensagens de erro são gerenciadas de forma simplificada, onde existe um parâmetro que define o formato da mensagem de log e o nome do gerenciador de log, além de outros pequenos parâmetros para identificação de onde saiu o log que foi registrado.

Existem alguns métodos nessa biblioteca que auxiliam no tratamento de erro de \textit{stack overflow}, onde esses métodos monitoram de onde saiu a execução do elemento que chamou essa função. Um exemplo é o caso do tratamento de erro do \textit{stack overflow} na criação de um dado de inserção da classe de criação de banco de dados, onde se porventura durante o tratamento de erro que faz com que a classe seja chamada recursivamente algumas vezes até que consiga retornar um valor válido. 

O tratamento de erro acontece na etapa da chamada de uma nova recursão para evitar um esgotamento de memória, já que caso chegue a um valor máximo de execuções, que é verificado pelo método nativo do Python chamado \textit{inspect}, é chamado o encerramento da recursão e simplesmente em vez de retornar um valor de inserção é retornado um valor nulo.

Além de saber qual a função que a chamou, esse método possui uma outra implementação, onde retorna toda a pilha de chamadas do algoritmo, com isso permitindo que um tratamento de erro mais complexo seja possível sem grandes alterações no código.


%\subsection{paralelLib}
%\label{subsec:paralelLib}
\subsec{paralelLib}
Essa biblioteca possui duas implementações diferentes de paralelização, utilizando a classe threading \cite{threading}, e a classe multiprocessing \cite{multiprocessing}. A classe threading utiliza \textit{threads} para a paralelização de execuções, esse método foi inicialmente utilizado por ser de simples implementação e monitoramento da execução.

Entretanto, devido às limitações de segurança descritas em \myref{subsubsec}{Erro do processamento paralelo de threads} essa classe, foi escolhida uma implementação utilizando a classe multiprocessing, onde essa classe cria um grupo de subprocessos para o código que irão ser responsáveis pela execução do algoritmo selecionado.

Ambas classes implementadas, a derivada de threading e a derivada de multiprocessing, foram feitas de formas análogas e de simples substituição em código de uma para a outra de acordo com a necessidade, sendo assim funcionam de forma parecida. Além disso as duas funcionam com uma classe de gerência chamada paralel, no caso de threading sendo paralel\_thread e no caso de multiprocessing sendo paralel\_subprocess, assim como classes \textit{worker} com nomes semelhantes, worker\_thread e worker\_subprocess.

As classes de gerência funcionam gerando um array de objetos da classe  \textit{worker}, sendo cada um equivalente a uma thread ou subprocesso da execução paralela, respectivamente. Também existe um parâmetro, que pode ser apenas um elemento ou array, com as funções que serão executadas no processamento paralelo, além disso gera um objeto que contém os elementos que serão processados durante o processamento paralelo.

Ademais, esses objetos são passados como kwargs, que são os parâmetros passados de forma padronizada, das funções que serão executadas, podem ser aceitas várias funções desde que sigam apenas um padrão em todas as entradas. Essas funções devem ter exatamente as mesmas entradas de dados, no caso utilizado no algoritmo, cada função é a referência de uma função associada a um objeto com uma credencial diferente para a comunicação com o banco de dados, de forma que não seja executada mais de uma comunicação com uma credencial por vez.

Os \textit{workers} vão ciclando as funções a medida que vão iterando pelo array dos parâmetros, a medida que as funções são executadas, os objetos contendo os parâmetros vão sendo removidos do array um por um logo antes de ser usado, dessa forma garantido que uma operação não será feita duas vezes. Isso é importante visto que o propósito desse algoritmo é simular a mesma quantidade de operações em bancos de dados diferentes.

Se um elemento fosse utilizado mais de uma vez esses valores se alterariam, caso um dos elementos já tenha sido usado e o \textit{worker} tente usá-lo de novo, um erro vai ocorrer, o \textit{worker} vai ignorar esse elemento e tentará o próximo disponível, e caso o número de elementos seja menor que 1 ele irá então terminar o loop e parar a execução desse \textit{worker}. Após o fim da execução de todos os processos paralelos a classe parallel terminará sua execução e retornará os dados pedidos.

O objeto de gerência, se for requisitado, irá retornar os resultados das operações onde o processamento paralelo foi pedido, porém, para os propósitos deste algoritmo isso não foi necessário nenhuma vez, sendo assim essa funcionalidade está incompleta. Além disso ele pode retornar o tempo gasto por cada \textit{worker}, essa função foi a usada para calcular o tempo gasto.

Essa biblioteca demandou muito do tempo de desenvolvimento, devido aos diversos problemas possíveis de acontecerem durante o manuseio de aplicações paralelas. Uma modificação em relação à classe nativa do Python foi a implementação de um time out para a execução dessas funções, de forma que as operações mesmo que travem não apresentam problema no benchmark.

Esse fator foi um problema percebido que impactava muito na velocidade do benchmark, gerando subprocessos zumbis, já que uma operação do banco de dados acabava levando mais tempo que o necessário, causando problemas na medição de tempo do benchmark, e em algum momento os subprocessos acabavam gerando um problema de travar um ao outro devido a essa demora. Entretanto, caso isso ocorra, o elemento utilizado voltará à lista de elementos a serem executados e será chamado pelo próximo \textit{worker} livre.

%\subsection{monitorContainer}
%\label{subsec:monitorContainer}
\subsec{monitorContainer}
Essa biblioteca se baseia na biblioteca \myref{sec}{Biblioteca PSutil} e utiliza o método \textit{eval} para executar um comando de console Python declarado como um \textit{string}, fazendo com que os dados consultados do hardware onde o contêiner está sendo rodado seja facilmente trocado.

Ademais, essa biblioteca faz a consulta a partir de um arquivo JSON contendo os comandos baseados no \textit{eval} e o filtro de retorno que será aplicado aos resultados deste comando, simplificando assim o trabalho da análise de logs. A biblioteca ainda implementa o uso da biblioteca \myref{subsec}{loggingSystem} para poder enviar todos eles de forma automática para o Logstash ou para um arquivo de log local.

%\subsection{gerenciadosDeBD}
%\label{subsec:gerenciadosDeBD}
\subsec{gerenciadorDeBD}
Essa biblioteca é feita para que a gerência das conexões do banco de dados seja feita mais facilmente, visto que tem as funções adaptadas para serem iguais tanto para o MariaDB, quanto para o Postgres. Além disso existem tratamentos de erro customizados para todos os erros que foram observados durante a etapa de desenvolvimento. Dessa forma todos os erros que acontecem são tratados de forma similar, mas como as bibliotecas são diferentes e desenvolvidas por entidades diferentes, várias das correções de erros e funcionamentos também são diferentes.

Um exemplo simples disso é que para a biblioteca do Postgres executar um arquivo SQL só precisa do \textit{string} contido nele, mas na biblioteca do MariaDB precisa ser lido e executado linha a linha. Na biblioteca criada foram feitas as adaptações, para que independente do banco de dados usado, a função de executar um arquivo SQL apenas precisa do caminho do arquivo no sistema. Pois assim ela já abre o arquivo e o executa com as devidas alterações para cada biblioteca de banco de dados utilizada.

Alguns outros comandos são feitos no Postgres, por exemplo existe uma função de rollback, que é usada para desfazer alguma operação quando ela é identificada como erro em uma comunicação com o banco de dados. Não existe algo equivalente para o MariaDB, sendo assim a classe criada trata esse erro tentando executar mais 5 vezes a operação, para garantir que não foi algum erro de conexão aberta. Após essas 5 vezes o algoritmo considera que a operação inserida está com problema e a ignora.

O gerenciadorDeBD serve principalmente para essa generalização, além disso ele realiza a execução de comandos SQL e gerencia a consulta e criação de usuário. Dessa forma essa biblioteca simplifica muito a interação com o banco de dados. Essa simplificação chega num ponto em que se tornou possível a paralelização, visto que uma adaptação para isso é bastante complexa caso o código não tenha sido pensado para este fim desde o começo. Sendo assim o algoritmo modularizado acabou fazendo possível a implementação de forma paralela.

Ademais, a biblioteca ainda faz com que seja possível consultar o SQLite dos dados gerados diretamente na comunicação com o banco de dados final. Isso faz com que o algoritmo economize mais memória RAM, mas faz também com que ele consuma por mais tempo disco e CPU, mas não necessariamente em maior quantidade. Isso faz com que tenha sido necessário maior controle da quantidade de \textit{threads} na execução do benchmark, pois como o CPU é usado por mais tempo, isso poderia causar um travamento da máquina que está gerando os testes e uma subsequente inconsistência dos dados de benchmark gerados.

Essa situação de travamento ocorreu durante uma parte dos testes onde a máquina que executou os testes travou, mas os servidores nos quais os testes estavam sendo feitos continuavam funcionando e respondendo normalmente. O que ficou claro com isso, já que nos gráficos os servidores não estavam rodando os containers, porém ao mesmo tempo não iniciavam a próxima etapa de testes. Isso aconteceu pois os \textit{threads} para a máquina de testes foram colocados para ser o máximo possível para que não apresentasse erros, o que a colocou o tempo todo bem próximo do limite e em algum momento ela ultrapassou esse limite, o que resultou nesse travamento.


%\subsection{geradorDeSql}
%\label{subsec:geradorDeSql}
\subsec{geradorDeSql}
Essa é a classe referida na seção \myref{sec}{Geração de dados}, todas as informações detalhadas dela estão descritas nessa seção.

%\section{software de benchmark}
%\label{sec:software de benchmark}
\sec{Software de benchmark}
O benchmark foi feito utilizando um software desenvolvido para o propósito deste teste, ele está mais para um software que gera estresse na máquina na qual está rodando o banco de dados por meio de várias execuções de comandos direto no banco de dados. O software foi projetado levando em conta que as maquinas onde ele seria realizado estariam rodando containers Docker  desenvolvidos para os propósitos desta pesquisa, eles estão descritos em \myref{sec}{Contêineres docker}. O software de benchmark funciona carregando e construindo os dados a partir dos dados inseridos no arquivo SQLite. Os testes de estresse foram testados de algumas formas:
\begin{itemize}
\item Primeiramente o algoritmo inicia o contêiner que contém o banco de dados que está sendo analisado;
\item Após isso é inserindo uma quantidade definida de operações que é lida de forma sequencial do banco de dados inicial;
\item Em seguida são executadas essas operações até que terminem, depois disso o mesmo procedimento é feito para a outra máquina;
\item Após isso o contêiner com o banco de dados é desligado e o próximo tipo de banco de dados é iniciado nas duas máquinas, e o processo se repete.
\end{itemize}

A outra forma é um pouco mais rápida e eficiente em relação a execução das múltiplas máquinas, isso se dá pelo fato de que o processo de inserção é feito de forma paralela, sendo assim as inserções dos bancos de dados em todas as máquinas são feitas simultaneamente, mas também de forma sequencial e o funcionamento é muito parecido com o da anterior:
\begin{itemize}
\item Inicialmente é criada uma thread para cada máquina;
\item Em cada thread é lido de forma sequencial as operações que serão inseridas;
\item O container de banco de dados escolhido é iniciado na máquina associada a thread;
\item As operações são executadas seguindo a lista de operações;
\item Assim que as operações acabam em todas as \textit{threads} o contêiner do banco de dados é parado; 
\item O algoritmo aguarda a outra thread iniciar para poder começar o mesmo procedimento para o outro tipo de banco de dados.
\end{itemize}

A terceira forma é parecida com a segunda, diferente apenas na forma como as operações são executadas, a partir desse método as operações são inseridas de forma paralela;
\begin{itemize}
\item Cada thread de cada máquina possui uma quantidade definida de \textit{threads} filhas;
\item As \textit{threads} filhas executam as operações a partir do que existe em uma Queue de elementos;
\item Após esse ponto o procedimento é igual ao teste descrito anteriormente, do terceiro tópico em diante, com a diferença de a Queue não ser tratada como uma lista sequencial e seus elementos podem ser readicionados em caso de erro como descrito em \myref{subsec}{paralelLib}.
\end{itemize}

A Queue de elementos não garante que uma operação será executada, visto que pode depender de uma operação que ainda não foi executada. Isso é possível em alguns casos raros, assim como num ambiente real, onde um funcionário de uma empresa pode editar um elemento ao mesmo tempo que outro edita o mesmo elemento, em ambientes reais existem tratamentos para que isso não ocorra.

No entanto, no ambiente desses testes, nenhum tratamento para impedir isso foi feito, exatamente para simbolizar o pior cenário possível para uma aplicação com comunicação com bancos de dados.

%\section{geração de dados}
%\label{sec:geração de dados}
\sec{Geração de dados}
Todos os dados foram gerados para um SQLite projetado para ser simples de aceitar qualquer formato de dados para qualquer tabela que fosse descrita no arquivo que define os dados que serão gerados, o funcionamento deste algoritimo de geração é descrito aqui. \newline

 Primeiramente, os dados foram gerados em SQLite, pois caso houvesse algum problema e o computador que estava gerando os dados fosse abruptamente desligado, o que havia sido previamente gerado não seria perdido. Desta forma, economiza memória RAM, já que não é necessário carregar o arquivo todo quando fosse necessária uma consulta de dados, e tempo uma vez que esta etapa é a mais demorada da geração de dados, como descrito em\myref{subsec}{SQLite}.

O código consegue gerar os dados de 3 formas, mas apenas a última é realmente utilizada, as outras duas foram feitas para propósito de testes, sendo elas:
\begin{itemize}
\item Gerar uma quantidade X de dados de uma tabela específica;
\item Gerar uma quantidade X de cada tipo de dado para cada tabela do banco de dados final;
\item Gerar uma quantidade aleatória de cada tipo de dado para as tabelas do banco de dados final até atingir o total de operações informadas.
\end{itemize}

Todos esses tipos de geração são feitos pela mesma função que são alterados pelos parâmetros passados a ela, sendo assim, a função prioriza os parâmetros referentes aos três tipos de geração.

Além dos parâmetros relacionados aos tipos de geração informados, existem os parâmetros relacionados a biblioteca faker e uma lista que define quais tipos de operação, descritos em \myref{subsec}{Tipos de dados gerados}, que define, caso não esteja vazia, quais os tipos de dados que serão gerados. Isso foi importante pois utilizou-se o tipo de criação de dados de inserção na primeira etapa de testes, como descrito em \myref{subsec}{Teste de tempo de operação de inserção no BD}, antes de gerar os dados dos outros tipos.

Devido às limitações intencionais, o valor total de operações inseridas em cada execução deve considerar a quantidade da execução anterior. Já que o algoritmo verifica apenas a quantidade total de elementos cadastrados no SQLite, ou seja, caso sejam requisitadas 3 tipos de geração diferentes teriam que ser passadas da seguinte forma:
\begin{itemize}
\item Uma geração da quantidade X de dados, resultando em X dados gerados;
\item Uma geração de Y dados, sendo que Y é igual a X+A, resultando em Y dados gerados;
\item Uma geração de Z dados, sendo que Z é igual a X+A+B, resultando em Z dados gerados;
\end{itemize}

Sendo assim, os dados gerados teriam que ser algo como X=5, Y=10 e Z=15, onde cada etapa apenas teriam 5 dados adicionados ao SQLite.

A geração de dados inicialmente foi pensada para rodar em um servidor de forma sequencial, isso simplesmente porque não teria processamento paralelo inicialmente no software. Entretanto quando isso se tornou necessário também foi possível utilizar o código previamente existente da geração de dados para a geração de forma paralela. Isso se mostrou uma grande vantagem para a etapa de desenvolvimento, onde vários bancos de dados de testes foram gerados para que fosse garantido que todas as partes desenvolvidas do software estivessem funcionando corretamente, devido a forma como isso foi pensado.

O grande limitador da velocidade de geração de dados é o fator randômico, devido a toda a recursão que ocorre em consequência aos tratamentos de erros. Além disso o outro maior limitador é a velocidade da mídia de armazenamento, visto que a aplicação SQLite3 aceita apenas uma inserção por vez no banco de dados de forma paralela, isso faz com que quanto mais rápido fosse possível a escrita em disco, mais rapidamente esse fator limitador era deixado de lado. 

Devido ao fato de durante os testes da etapa de desenvolvimento todos terem sido usando um SSD para o salvamento desses dados isso não impactou em quase nada na velocidade de geração dos dados, mas na etapa de execução de testes isso se mostrou um problema, já que no dispositivo usado para executar o algoritmo dos testes o HD mecânico fez com que ocorresse diminuição da velocidade de leitura e escrita, sendo assim mesmo na operação de leitura, onde podem ser feitas várias leituras concorrentemente isso impactasse na performance e gerassem alguns erros como descrito em \myref{subsubsec}{Erro de leitura pela falta de velocidade de disco}.

%\subsection{tipos de dados gerados}
%\label{subsec:tipos de dados gerados}
\subsec{Tipos de dados gerados}
Foram selecionadas para os teste apenas as operações mais utilizadas por um banco de dados:
\begin{enumerate}
\item Inserção de um novo dado;
\item Leitura completa de todos os dados de uma tabela;
\item Busca de elementos filtrados em determinada tabela;
\item Busca de apenas alguns dados de elementos filtrados em determinada tabela;
\item Edição de elementos;
\item Deleção de elementos filtrados.
\end{enumerate}

Antes de ser gerada, a operação de inserção passa por vários processos de tratamento de erro para se certificar que não houve dependência alguma que não foi gerada, como por exemplo, gerar uma cidade sem existir um país cadastrado.

Esse foi um dos motivos de ter sido utilizado um arquivo SQLite ao invés de outro método de armazenamento, pois dessa forma, pode-se executar essa consulta de dependência de forma rápida e apenas retornar índices válidos para associações de tabela.

%\subsection{alimentação do algoritmo}
%\label{subsec:alimentação do algoritmo}
\subsec{Alimentação do algoritmo}
O algoritmo de geração de dados funciona de forma que qualquer banco de dados possa ser utilizado para ter seus dados gerados, é necessário apenas utilizar um arquivo JSON e seguir um determinado padrão que é composto por:\newline
\begin{itemize}
\item Uma tag com o nome de uma tabela do banco de dados; 
\item Dentro dela uma estrutura JSON com:
\begin{itemize}
\item Uma tag com o nome da coluna;  
\item Seu conteúdo em um array de string contendo;
\begin{itemize}
\item O primeiro tipo de dado;  
\item Outros valores adicionais para a geração.
\end{itemize}
\end{itemize}
\end{itemize}

Dentro do algoritmo existem vários tipos de dados aceitáveis, tais como id, nome, associação e timestamp, sendo que cada um deles possui um tipo bem definido pelo seu nome, mas o único que vale a pena citar seu funcionamento é a associação.

Como descrito em \myref{subsec}{SQLite} existe uma tabela do SQLite contendo as quantidades de elementos associados a cada tabela do banco de dados final. O tipo de associação vai pegar esse valor e usar uma função de seleção randômica para que seja escolhido um elemento de id existente no intervalo descrito, e caso não exista algum elemento dessa tabela, por um tratamento de erro é gerado um elemento para ela, permitindo o funcionamento da associação no novo elemento que foi criado.

Os únicos dados que não são passados para o algoritmo funcionar são: o país que deve ser gerado os dados de acordo com a \myref{sec}{Biblioteca Faker} e a quantidade de dados que devem ser gerados, ambos sendo necessários de serem inseridos na chamada da função. Os outros dados relevantes referentes ao funcionamento da chamada da função estão em \myref{sec}{Geração de dados}\newline

%\subsection{SQLite}
%\label{subsec:SQLite}
\subsec{SQLite}
O banco de dados do SQLite foi projetado para ser totalmente maleável e modular, de forma que não teriam que ser geradas várias tabelas para os vários tipos de dados do benchmark. Além diso, para facilitar o desenvolvimento foi pensado a seguinte estrutura, sendo uma tabela de índices e uma tabela de operações. A tabela de índices possui apenas 3 colunas, uma id, uma com o nome da tabela e uma com o total de elementos dessa tabela. Já a outra tabela é um pouco mais complexa e está descrita a seguir:
\begin{itemize}
\item A tabela de operações a ser executada é constituída de uma coluna inteira para o tipo de operação que será realizada, de acordo com \myref{subsec}{Tipos de dados gerados};
\item Uma coluna é uma string contendo o nome do banco de dados que será executada a operação;
\item Uma coluna inteira para, se for necessário, conter o id no banco de dados do elemento trabalhado na operação, sendo que no caso de uma inserção é o id do novo elemento, por exemplo;
\item Uma coluna de texto, onde serão inseridos valores adicionais necessários para a execução da operação, como os parâmetros de quais colunas devem ser atualizadas em um update. Nesse campo, os dados inseridos são salvos em formato JSON para facilitar o trabalho com a linguagem Python, visto que existe uma conversão direta de string JSON para o tipo dictionary do Python;
\item Uma coluna de texto seguindo a mesma ideia da coluna anterior, sendo que esses dados são os dados obrigatórios de qualquer operação, onde dependendo da operação os dados contidos são diferentes.
\end{itemize}

Dessa forma, independente se é apenas uma operação de listagem completa, que só necessita de ter preenchida a coluna com o nome do banco de dados e a coluna com o tipo de operação ou se for uma operação de update onde todos os campos podem estar preenchidos, o banco de dados SQLite consegue lidar de forma rápida e segura com qualquer uma das operações e dados gerados pelo algoritmo. Ademais, foi cogitado o uso de outras estruturações da tabela de operações, mas essa foi a mais simples de ser implementada que era válida para todas as operações, além de ser a mais reaproveitável.


%\section{containers docker}
%\label{sec:containers docker}
\sec{Contêineres docker}
Os containers Dockerforam criados a partir do sistema Alpine Linux, no qual foram feitos 2 contêineres diferentes, um para o MariaDB e outro para o Postgres. Os contêineres se certificam de criar o banco de dados de forma correta durante a inicialização dele, após isso o contêiner, durante a sua inicialização, se certifica de que o Python está instalado e tudo necessário para que o daemon, descrito em \myref{sec}{Daemon de monitoramento}, funcione, após isso o banco de dados do contêiner é finalmente iniciado. Essa forma de instalação do daemon certifica que tanto o interpretador Python3 quanto as bibliotecas usadas estão sempre atualizadas todas as vezes que o contêiner é instanciado.

Entretanto além de ocorrer algum problema desconhecido que resultou em uma falha nesse procedimento, sua necessidade se provou inexistente, visto que seria verificada a saúde do contêiner antes da inicialização das execuções dos testes, mas apenas deixar um delay de alguns segundos bastou para que não ocorressem erros relacionados a isso. 

Containers baseados no alpine possui uma especificidade para a arquitetura armhf, presente no Orangepi PC +, só funcionam corretamente sem grandes modificações do sistema até a versão 3.12, devido a uma mudança no método como o sistema operacional manipula o relógio interno, o que quer dizer nenhuma conexão com chave de encriptação funciona nele, sendo assim a versão utilizada foi a versão 3.12.

Outra especificidade do alpine é que ele é um dos poucos sistemas Linux que não possui o GCC incluso como pacote padrão do sistema, fazendo com que o Python tenha alguns problemas ao instalar algumas bibliotecas, dentre elas o PSutil, que foi utilizado pelo \myref{sec}{Daemon de monitoramento}, para solucionar isso foi necessário instalar um programa chamado linux-headers que é um conjunto de programas e bibliotecas padrões das distribuições Linux, além do próprio GCC, após a adição desses programas o contêiner conseguiu funcionar corretamente.

%\section{daemon de monitoramento}
%\label{sec:daemon de monitoramento}
\sec{Daemon de monitoramento}
Este daemon monitora os status da máquina na qual está rodando, seja uma máquina física ou um container docker. Para esse fim é utilizada a biblioteca \myref{sec}{Biblioteca PSutil} que é a principal biblioteca Python quando se trata de monitoramento de hardware. O daemon utiliza também uma biblioteca feita para simplificar o tratamento de log, uma das funcionalidades contidas nesta biblioteca é a comunicação com o aglutinador de logs Logstash do\myref{sec}{Elasticsearch monitoring stack}, isso feito em cima da biblioteca python-logstash.

O arquivo de configuração do daemon é um JSON constituído de duas partes, os parâmetros para se conectar ao Logstash e os dados que serão coletados da máquina local, esses dados coletados serão enviados para o servidor Logstash onde serão trabalhados. 
O daemon envia esses dados com um intervalo de 0.1 segundos, este intervalo de tempo existe para que não sejam enviados dados errados de CPU para o servidor de log, uma vez que é comum o erro de se enviar 0\% de uso de CPU caso as consultas sejam feitas muito rapidamente.

No entando, esse intervalo de tempo poderia ser aumentado para diminuir o estresse na máquina que está rodando ele e diminuir um pouco o estresse do servidor de coleta de logs, que com as máquinas usadas não se provou necessário, além disso diminui a quantidade de dados no pós processamento de resultados descritos em \myref{sec}{Processamento de resultados}.

%\section{ambiente de desenvolvimento}
%\label{sec:ambiente de desenvolvimento}
\sec{Ambiente de desenvolvimento}
O ambiente utilizado foi dividido em duas partes, programação remota e local. Na  primeira as execuções foram feitas em um servidor baseado em Raspberry PI 4 e na programação local foram feitas em duas máquinas diferentes, sendo computadores locais. A programação remota se provou bem útil quando houve a necessidade de troca de computador ou sistema operacional, facilitando ainda a implementação de um servidor unificado de análise de log, pois era mais simples a comunicação entre o código executado e o servidor\myref{sec}{Elasticsearch monitoring stack}.

Foram utilizados Visual Studio Code e DBeaver para o desenvolvimento do \myref{sec}{Software de benchmark}, e também foi utilizada uma implementação de um algoritmo de processamento para \myref{sec}{Elasticsearch monitoring stack} com a finalidade de gerar os gráficos e tabelas apresentadas neste trabalho. Os dois primeiros foram escolhidos, dentre outros motivos, por serem \textit{open-source} e estarem disponíveis tanto para Linux quanto para Windows, visto que ambos sistemas operacionais foram utilizados para o desenvolvimento de acordo com a necessidade.

Para o controle de versão foi utilizado o Git, deixando registrado todo o histórico de alterações do programa, o que se mostrou bem útil para o rastreio de erros ocorridos durante o longo tempo de desenvolvimento do código. Para os testes iniciais do código foram utilizados containers Docker não limitados durante a etapa de implementação dos scripts do DBbench, que demandam a existência dos servidores dos bancos de dados sendo executados, ao contrário do restante do desenvolvimento da aplicação, que não interagiu diretamente com os bancos de dados.

A geração do banco de dados inicial, descrito em \myref{subsec}{SQLite}, foi feita completamente em um Raspberry Pi 4 com um pequeno overclock, essa geração foi feita durante alguns dias, visto que o \myref{sig}{SBC}, de acordo com \myref{sec}{Testes de tempo}, consome menos energia e, devido a sua configuração pré existente, possui um acesso \textit{headless} mais simplificado que as outras máquinas utilizadas.


\sec{Processamento de resultados}
Os dados coletados pelo \myref{sec}{Elasticsearch monitoring stack} foram extraídos para o formato CSV e os dados desnecessários foram removidos, mesmo que durante o processo de \myref{sec}{Daemon de monitoramento} apenas os dados selecionados foram coletados, já que posteriormente alguns destes dados provaram ser irrelevantes para os resultados deste trabalho. Após esse processamento inicial foi usado um segundo processamento para eliminar duplicatas e para separar em quatro arquivos diferentes os resultados, um para cada banco de dados em cada máquina. A última etapa é a plotagem dos gráficos utilizando a biblioteca Altair que gera gráficos SVG que devido a sua natureza vetorial se mostraram de melhor manuseio já que mesmo aumentando o zoom ela não se distorcia.

Além disso, foi feita uma verificação manual para certificar-se que os dados coletados estavam corretamente associados ao servidor relativo a elas.
Estes dados são os responsáveis por distinguir qual a ordem que os dados foram armazenados no array de tempos gastos do arquivo JSON de resultados do benchmark, pois caso o índice 1 seja o \myref{sig}{AMD64} ele se manterá o mesmo até o final da execução, o que ocorreu na maioria dos testes realizados.

Uma última etapa de processamento foi feita para a geração de vários arquivos pequenos ao invés de um único arquivo grande. Foi detectado que todas as vezes que o container reiniciava, os dados de tráfego de rede retornavam a zero.
Sendo assim, todas as vezes que o algoritmo identificou que o valor de rede era menor que o valor anterior do mesmo container ele dividia a seção anterior para um arquivo novo. 

O resultado obtido por esse procedimento formam mais de 600 arquivos CSV diferentes para o teste descrito em \myref{subsec}{Teste de tempo de operações paralelas}, cada um sendo correspondente a uma interação.
Após isso o algoritmo de processamento juntava uma quantidade máxima definida de testes por arquivo SVG, resultando nas 12 imagens de cada container apresentadas nos anexos deste trabalho.

\ch{Resultados}
Durante este capítulo serão abordados os resultados obtidos pelos testes descritos em \myref{ch}{Testes}.

\sec{Resultados de inserção fracionada}
O resultado encontrado no primeiro teste de manipulação de bancos de dados, que foram gerados de acordo com o que foi descrito em \myref{subsec}{Teste de tempo de operação de inserção no BD}, foi que nestes dados gerados comprovadamente não existe ocorrência de erros como foi descrito em \myref{sec}{Geração de dados}. De acordo com os dados coletados pelo \myref{sec}{Contêineres docker} em todos os núcleos disponíveis para o contêiner existe uma variação de porcentagem de uso, como descrito nos arquivos de logs anexos e na tabela \myref{imagem}{Resultados inserção fracionada}.

Essa variação ocorre no computador \myref{sig}{ARM} pelo fato de ter mais núcleos físicos, devido a esse fator é mais complicado de ser avaliada a porcentagem de uso de CPU já que o Docker distribui a carga de uso de forma a distribuir todos os processos em todos os núcleos disponíveis mas ainda assim mantendo um uso total limitado ao que foi definido. Em relação ao uso de RAM, ele se manteve mais constante na arquitetura \myref{sig}{ARM}, o uso de RAM é bem menor que no \myref{sig}{AMD64}, nas duas arquiteturas a diferença do uso de RAM é de cerca de 5\%, onde com o MariaDB a variação faz com que o uso de RAM seja ligeiramente maior no\myref{sig}{AMD64} que com o Postgres. A velocidade de disco de escrita é em torno de 500kbps para o \myref{sig}{ARM} e 3Mbps para o \myref{sig}{AMD64}, já para o MariaDB é de 3.5Mbps para o \myref{sig}{ARM} e 9.5Mbps para o \myref{sig}{AMD64}. Levando em conta os dados apresentados não aparenta existir uma limitação de velocidade de escrita no \myref{sig}{ARM} devido as especificidades do seu \textit{chipset}, isso tendo em vista que em ambas as maquinas a velocidade de 3Mbps de escrita, o que sugere que as diferenças de velocidade se devem mais a otimizações de \textit{build} das aplicações para cada arquitetura e os \textit{Kernel} das maquinas usadas.

De acordo com a descrição de \myref{ch}{Testes}, essas variações das velocidades são provavelmente devido às otimizações das arquiteturas para cada banco de dados. Já a velocidade de leitura de disco é praticamente 0\% o tempo todo, devido a pouca necessidade de informações consultadas dos dados cadastrados pelo algoritmo de benchmark.

A principal diferença identificada em relação aos testes, além do previamente informado, é o tempo. Já que em torno de 60\% dos casos o Postgres \myref{sig}{ARM} é mais rápido comparada com o \myref{sig}{AMD64}, sendo que o MaridaDB, na arquitetura \myref{sig}{AMD64}, possui 85\% de melhor desempenho.

\imagemsvg{Resultados inserção fracionada}{insercao fracionada/insercao tempos.svg}{valores de tempo do benchmark de inserção fracionada, o eixo X corresponde a quantidade de elementos totais inseridos e o eixo Y corresponde ao tempo gasto em segundos para cada 5000 elementos inseridos.}


\begin{easyTableAuto}{Dados encontrados de uso de hardware teste inserção, nesta tabela são apresentados os valores médios e máximos de CPU e RAM em cada ambiente testado}{ Container & Média de CPU & Valor máximo médio de CPU & Média de RAM & Valor máximo médio de RAM }{5}{p{.15\textwidth}|p{.15\textwidth}|p{.2\textwidth}|p{.15\textwidth}|p{.2\textwidth}}
%nome da tabela e label
%cabeçalho
%quantidade total de colunas
%formatação da tabela
 Postgres ARM & 8 & 25 & 27 & 28 \\ \hline
 Postgres AMD & 18 & 27 & 30 & 30 \\ \hline
  MariaDB ARM & 8 & 16 & 34 & 35 \\ \hline
  MariaDB AMD & 18 & 27 & 34 & 35 \\ \hline

\end{easyTableAuto}
%lable
%arquivo
%caption
\sec{Resultados de operação completa fracionada}
Ao contrário do que era esperado, os valores gerados pelo \myref{sec}{Software de benchmark} geraram de forma igualmente distribuída, ou seja, os dados foram gerados com uma distribuição de aproximadamente 20\% para cada tipo de operação. Isso ocorreu devido a algum motivo desconhecido nas operações de tipo 5, como descrito em \myref{subsec}{Tipos de dados gerados}. Além disso, todos os outros dados gerados ocorreram como o esperado, onde são feitas \textit{queries} de seleção, pesquisa e inserção.

Um fator interessante a se destacar é que, apesar da distribuição de operações ter sido homogênea, o custo de uso de hardware não foi, pois em certos momentos houve um grande uso de CPU, mas sempre próximos dos valores apresentados em \myref{sec}{Resultados de inserção fracionada}. Na maioria dos casos houve um maior uso de RAM que o apresentado no dito teste e um fator a se destacar é que os primeiros ciclos de testes funcionam da seguinte forma:

\begin{itemize}
\item A maior parte, cerca de 60\% dos dados, foi de inserção e o restante de operações variadas, diferente do esperado, provavelmente devido a algum problema durante a gerência dos arquivos salvos na máquina, não no algoritmo em si, sendo comprovado nos próximos nove ciclos.
\item A partir do 11° ciclo, no Postgres, houve um aumento significativo de RAM, os valores que variam entre 18 a 30\% começaram a variar de 18 a 53\% no \myref{sig}{AMD64} e no \myref{sig}{ARM} a variação saiu de 20\% a 34\% para 20\% a 52\%.
\end{itemize}
Os tempos apresentados por esse teste estão descritos em \autoref{imagem:resultados todos fracionada}
\imagemsvg{resultados todos fracionada}{todos fracionada/todos tempos.svg}{Valores de tempo do benchmark de operações mistas fracionadas, o eixo X corresponde a quantidade de elementos totais inseridos em função de valores na notação de $10^6$ e o eixo Y corresponde ao tempo gasto em segundos para cada 5000 elementos inseridos}

Contudo, um fato interessante ocorreu em uma etapa do teste, onde aparentemente a máquina que estava executando o teste travou e voltou a funcionar em seguida, durante um intervalo de tempo nenhum dos containers estavam ativos, sendo assim, nenhuma operação estava sendo realizada neles.
%lable
%arquivo
%caption

\begin{easyTableAuto}{Dados encontrados de uso de hardware teste completo, nesta tabela são apresentados os valores médios e máximos de CPU e RAM em cada ambiente testado}{ Container & Média de CPU & Valor máximo médio de CPU & Média de RAM & Valor máximo médio de RAM }{5}{p{.15\textwidth}|p{.15\textwidth}|p{.2\textwidth}|p{.15\textwidth}|p{.2\textwidth}}
%nome da tabela e label
%cabeçalho
%quantidade total de colunas
%formatação da tabela
 Postgres ARM & 16 & 33 & 61 & 63 \\ \hline
 Postgres AMD & 9 & 45 & 62 & 63 \\ \hline
  MariaDB ARM & 16 & 23 & 30 & 30 \\ \hline
  MariaDB AMD & 9 & 16 & 35 & 35 \\ \hline
\end{easyTableAuto}

\sec{Problemas e erros}
Nesta seção foram descritos os erros mais marcantes durante os processos necessários para a obtenção dos resultados deste trabalho.

\subsec{Desenvolvimento do algoritimo}
Durante o processo de desenvolvimento do algoritimo, ocorreram diversos tipos de erros em praticamente todas as etapas. Os erros mais relevantes para este trabalho ocorreram durante a etapa de criação do SQLite, no processamento paralelo e na inserção dentro do banco de dados finais.

\subsubsec{Erros da geração do SQLite}

Como foi dito em \myref{sec}{Biblioteca Faker} existem vários parâmetros diferentes dependendo do país selecionado, o que se relaciona diretamente ao principal erro apresentado na etapa de geração do SQLite, pois mesmo tendo apresentado vários erros relacionados a geração, o mais relevante é em relação a seleção de países. Neste caso, o dado mais visível que apresenta este problema é a geração dos números de celular, visto que o Brasil é um dos poucos países que possuem essa opção dentre os existentes na biblioteca, o que torna este dado incompatível com a maioria dos países existentes na biblioteca.

No entanto, apesar de não ser o único problema encontrado, já que também existem problemas com os parâmetros de geração de endereço e alguns poucos dados de informações pessoais que ocorrem também pela não padronização dos dados gerados pela biblioteca Faker nos vários países que ela aceita.
Entretanto, todos esses erros podem facilmente serem corrigidos com tratamentos de erros internos em cada tipo de dado gerável pelo algorítimo, mas para os propósitos deste trabalho não se provaram proveitosos o bastante.

Contudo, a forma escolhida para solucionar esse problema foi retirar a funcionalidade de seleção de múltiplos países na geração de dados e apenas ser utilizada para o Brasil, já que estes erros destacados foram causados ao ser trocado o país selecionado para a geração de dados. Este erro afeta diretamente no algoritimo que gera os dados de testes e, sendo assim, para que o benchmark possa ser mais refinado e completo a correção deste erro seria interessante.

\subsubsec{Erro do processamento paralelo de threads}
Durante a etapa de processamento paralelo, seus problemas de desenvolvimento e erros se misturam com a etapa de inserção de dados no banco de dados final. Ambos foram desenvolvidos simultaneamente, apesar do processamento paralelo ter sido iniciado primeiro.

O processamento paralelo teve vários tipos de problemas, sendo que o primeiro foi a implementação original dele. No inicio foi pensado em utilizar a implementação de \textit{threads} ao invés de subprocessos, pois as \textit{threads} são mais fáceis de serem lidas caso ocorra um erro, já que os dados são comunicados diretamente pela classe Queue que é utilizada para compartilhar dados em tempo real entre várias instâncias de processamento durante a execução de um programa Python.

Este compartilhamento entre várias instâncias, descrito anteriormente, quer dizer que ela não faz distinção se as \textit{threads} foram ou não iniciadas pela mesma classe, uma vez que existam \textit{threads} elas compartilham as mesmas informações e espaço de memória. Isso foi identificado devido a árvore de processos necessária para que os testes fossem feitos de forma simultânea, já que ao se iniciar uma thread de um processo, essa thread não pode iniciar uma outra thread hierarquicamente inferior a ela, resultando em um problema da implementação da linguagem C sobre a qual a classe de processamento paralelo Queue foi construída.

De acordo com as documentações encontradas, o erro do processamento paralelo de \textit{threads} foi devido ao compartilhamento de endereços de memória na linguagem C chamada "double free or corruption (out)". Esse erro ocorre quando se remove ou adiciona alguma variável ou índice de vetor de um processamento paralelo, devido a forma como a biblioteca de processamento paralelo lida com os valores das operações para evitar execução dupla de algum valor. Isso não ocorre quando se utiliza a biblioteca Manager ao invés da Queue, porém essa biblioteca está associada oficialmente ao processamento paralelo de subprocessos, por isso a migração de \textit{threads} para subprocessos foi escolhida.

\subsubsec{Erro do processamento paralelo de subprocessos daemon}
Um dos problemas encontrados foi uma mensagem de erro chamada de "AssertionError: daemonic processes are not allowed to have children", esse erro impede que processos daemon tenham processos filhos, fazendo com que o processo de sub-processamento tivesse que ser convertido de daemon para um processo convencional. Esse fator pode ser problemático quando se quer fazer um monitoramento do tempo de forma mais direta, pois no modo daemon é possível rodar um código enquanto aguarda a execução dos processos daemon serem terminadas.

O método escolhido no final do desenvolvimento foi o de utilizar uma thread de processo não daemon para monitorar o tempo e somente utilizar daemons nos subprocessos filhos. Devido a essa modificação na implementação das \textit{threads} principais foi utilizado o método de esperar cada daemon terminar e registrar o tempo gasto individualmente em um elemento do array de tempo gasto. Essa thread retornava o tempo para a função que a chamou, o processo foi um pouco mais complexo de implementar que o previamente proposto, mas terminou com um resultado mais preciso quando não utilizava daemon.

\subsubsec{Rollback de dados no Postgres}
Outro erro marcante foi o de tratamento de rollback nas operações do Postgres. Devido a forma como as operações dos bancos de dados foram geradas, nenhuma das operações além da inserção e seleção é realmente válida, concluindo que, de acordo com os métodos de segurança e otimização do Postgres é necessário executar um rollback após a sua execução para evitar problemas. Desta forma, resulta entre outras coisas, em uma maior demora da execução do Postgres quando dados considerados inválidos são executados, o que inclui todas as operações de atualização e deleção geradas pelo algorítimo descrito em \myref{sec}{Geração de dados}, e também em uma maior necessidade de tratamento de erros relacionados ao Postgres da biblioteca \myref{subsec}{gerenciadorDeBD}. 
        
%Ademais, durante a etapa de desenvolvimento foi necessário reformular os tratamentos de erro de várias formas para que apenas os erros que realmente necessitavam de rollback tivessem a operação executada, já que devido a hierarquia do tratamento de erro da biblioteca psycopg2, alguns erros que ocorriam eram entendidos como necessários de serem executados os rollback em vez do tratamento correto, principalmente o erro de timeout da conexão ou de conexão fechada.

\subsec{Erros dos testes implementação final}
Durante a etapa de testes implementação final, ou seja, os testes para transferir a execução do algoritimo das máquinas utilizadas para programar para a máquina que os testes finais seriam feitos após a etapa de desenvolvimento, foi transferido das maquinas de teste, uma sendo um Ryzen 3 3200g com 16GB de RAM e um SSD de 480GB, e a outra um i7 7500u com 8GB de RAM e um SSD de 240GB para um servidor baseado no FX-6300 com 4GB de RAM e um HD de 160GB.

\subsubsec{Erro de leitura pela falta de velocidade de disco}
O HD da máquina dos testes finais possui velocidades limitadas pela sua tecnologia, SATA 2, impactando de forma considerável a execução dos testes, já que mesmo com a possibilidade de serem feitas diversas leituras simultâneas no SQLite, a tecnologia lenta de leitura de dados causava uma latência entre a requisição da consulta e todo o processo lógico e físico da leitura ocasionando um \textit{timeout} na biblioteca SQLite de tempos em tempos.

Uma solução para isso foi a substituição da mídia de armazenamento do arquivo SQLite, foi utilizado um pendrive Kingston 3.0 de 16GB com velocidades de leitura superiores, de quase o dobro da velocidade, e latência muito inferior que a tecnologia SATA 2. Após essa substituição não foi mais visto nenhum erro de \textit{timeout} durante os testes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                      Capítulo 6                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ch{Conclusão}
Após a realização e observação dos resultados dos testes realizados, foi constatado que a arquitetura \myref{sig}{ARM} é a mais eficiente em relação ao desempenho por watt consumido. Entretanto, os resultados de ambas foram bem próximas, quase iguais para cada tipo de banco de dados diferente, resultando em uma eficiência energética de cerca de 9 vezes maior para a arquitetura \myref{sig}{ARM}, visto que o computador \myref{sig}{ARM} utilizado consome 10w de energia enquanto o computador \myref{sig}{AMD64} utilizado consome 90w de energia, isso se considerarmos que o consumo de ambos foi de 100\% da capacidade das respectivas fontes o tempo todo.

Os testes realizados foram focados em manter ambos servidores com a mesma característica de carga e desempenho, sempre visando o objetivo final de fazer com que a maior parte dos dados coletados pudesse ser diretamente comparáveis, já que o mesmo número de núcleos, RAM, velocidade de disco e rede estavam disponíveis para ambas máquinas e o \textit{clock} delas também foi limitado para exatos 1ghz para que as contas finais de comparação fossem mais simples. Sendo assim pode-se considerar os testes como sendo testes 1 para 1 nas diferentes arquiteturas.

Os dados coletados apontam que a arquitetura \myref{sig}{ARM} possui uma porcentagem de uso de CPU mais baixa que a da arquitetura \myref{sig}{AMD64}, indicando uma otimização do ponto de vista dessa arquitetura. Essa diferença causou uma alguma demora de processamento na arquitetura \myref{sig}{ARM}, levando em conta que essa diferença é bem baixa, se tornando irrisória em relação às comparações de arquiteturas. A arquitetura \myref{sig}{AMD64} apresentou um consumo maior de RAM, sendo assim é possível indicar que os programas de banco de dados utilizados tiveram suas otimizações focadas em pontos diferentes. Os bancos de dados MariaDB e Postgres apresentam diferenças visíveis entre si em relação a uso de RAM e CPU como foi mostrado em \myref{sec}{Resultados de inserção fracionada} e \myref{sec}{Resultados de operação completa fracionada}.

Em resumo, levando em consideração as comparações feitas no ambiente utilizado, o benchmark resultou em dados estatisticamente iguais em ambas as máquinas utilizadas, resultando apenas numa diferença final apenas em relação ao consumo energético entre as máquinas que, considerando que ambas máquinas estão bem ultrapassadas em quesitos de eficiência energética para suas respectivas arquiteturas, torna a arquitetura \myref{sig}{ARM} a mais indicada para ambientes de servidor de bancos de dados, visto que pode resultar numa visível diferença de custos para a empresa que hospedar essas máquinas.



%\anexo{log tempo insercao}{teste insercao/valores_tempo_velocidade_benchmark_fracionado.json}
%\label{anexo:#1}
%\lstinputlisting{apendices/teste insercao/valores_tempo_velocidade_benchmark_fracionado.json}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                      REFERÊNCIAS                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\nocite{inteli5}
\nocite{m1vsi5}
\nocite{armv9}
\nocite{mariamongomysqlpostgre}
\nocite{mysqlPostgressqlserver}
\nocite{mysqlPostgres}
\nocite{dockerdoc}
\nocite{dockercomposedoc}
\nocite{psutil}
\nocite{portainer}
\nocite{multiprocessing}
\nocite{threading}
\nocite{SQLite3}
\nocite{logging}
\nocite{faker}
\nocite{dockerpython}
\nocite{elk}
\nocite{opipc}
\nocite{G405}
\nocite{E1}
\nocite{h3}
\nocite{Postgrespython}
\nocite{mysqlpython}
\nocite{histARMEmbarcados}
\nocite{ARMv6Manual}
\nocite{ARMhist}
\nocite{ciscxrisc}
\nocite{cisc}
\nocite{risc}
\nocite{armevolution}
\nocite{riscvscisc}
\nocite{codigo}
\bibliography{abntex2-modelo-references}

% ---
% Finaliza a parte no bookmark do PDF, para que se inicie o bookmark na raiz
% ---
\bookmarksetup{startatroot}%
% ---

% ---------------------------------------------------------------------------------------------
% ELEMENTOS PÓS-TEXTUAIS
% ---------------------------------------------------------------------------------------------
\apendices
\ch{apendices relacionados ao teste completo}
\sec{imagens}
o eixo x em todos os graficos é o tempo decorrido,no grafico de cima o eixo y é o uso de CPU,onde cada cor é um nucleo diferente,no grafico do meio o eixo y é o uso de RAM em porcentagem e no grafico de baixo o eixo y corresponde ao uso de disco em megabytes,ou bytes em escala de 10e6,onde em azul é a leitura de disco e em laranja é a leitura de disco
\subsec{todos tempos}
\apendicesvg{todos fracionada/todos tempos.svg}{valores tempo benchmark de operações mistas fracionadas,o eixo x corresponde a quantidade de elementos totais inseridos em função de 10e6  e o eixo y corresponde ao tempo gasto em segundos para cada 5000 elementos inseridos}
\pagebreak

\subsec{Resultados todos parte 0}
\apendicesvg{todos fracionada/uso do container container_mariadb_amd_limpo0.svg}{Intervalo 0 do uso de hardware do container  MariaDB na arquitetura amd64}%
\apendicesvg{todos fracionada/uso do container container_mariadb_armhf_limpo0.svg}{Intervalo 0 do uso de hardware do container  MariaDB na arquitetura armhf}%
\apendicesvg{todos fracionada/uso do container container_postgres_amd_limpo0.svg}{Intervalo 0 do uso de hardware do container Postgres na arquitetura amd64}%
\apendicesvg{todos fracionada/uso do container container_postgres_armhf_limpo0.svg}{Intervalo 0 do uso de hardware do container Postgres na arquitetura armhf}%
\pagebreak

%\subsec{Resultados todos parte 1}
%\apendicesvg{todos fracionada/uso do container container_mariadb_amd_limpo1.svg}{Intervalo 1 do uso de hardware do container  MariaDB na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_mariadb_armhf_limpo1.svg}{Intervalo 1 do uso de hardware do container  MariaDB na arquitetura armhf}%
%\apendicesvg{todos fracionada/uso do container container_postgres_amd_limpo1.svg}{Intervalo 1 do uso de hardware do container Postgres na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_postgres_armhf_limpo1.svg}{Intervalo 1 do uso de hardware do container Postgres na arquitetura armhf}%
%\pagebreak

%\subsec{Resultados todos parte 2}
%\apendicesvg{todos fracionada/uso do container container_mariadb_amd_limpo2.svg}{Intervalo 2 do uso de hardware do container  MariaDB na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_mariadb_armhf_limpo2.svg}{Intervalo 2 do uso de hardware do container  MariaDB na arquitetura armhf}%
%\apendicesvg{todos fracionada/uso do container container_postgres_amd_limpo2.svg}{Intervalo 2 do uso de hardware do container Postgres na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_postgres_armhf_limpo2.svg}{Intervalo 2 do uso de hardware do container Postgres na arquitetura armhf}%
%\pagebreak

%\subsec{Resultados todos parte 3}
%\apendicesvg{todos fracionada/uso do container container_mariadb_amd_limpo3.svg}{Intervalo 3 do uso de hardware do container  MariaDB na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_mariadb_armhf_limpo3.svg}{Intervalo 3 do uso de hardware do container  MariaDB na arquitetura armhf}%
%\apendicesvg{todos fracionada/uso do container container_postgres_amd_limpo3.svg}{Intervalo 3 do uso de hardware do container Postgres na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_postgres_armhf_limpo3.svg}{Intervalo 3 do uso de hardware do container Postgres na arquitetura armhf}%
%\pagebreak

%\subsec{Resultados todos parte 4}
%\apendicesvg{todos fracionada/uso do container container_mariadb_amd_limpo4.svg}{Intervalo 4 do uso de hardware do container  MariaDB na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_mariadb_armhf_limpo4.svg}{Intervalo 4 do uso de hardware do container  MariaDB na arquitetura armhf}%
%\apendicesvg{todos fracionada/uso do container container_postgres_amd_limpo4.svg}{Intervalo 4 do uso de hardware do container Postgres na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_postgres_armhf_limpo4.svg}{Intervalo 4 do uso de hardware do container Postgres na arquitetura armhf}%
%\pagebreak

%\subsec{Resultados todos parte 5}
%\apendicesvg{todos fracionada/uso do container container_mariadb_amd_limpo5.svg}{Intervalo 5 do uso de hardware do container  MariaDB na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_mariadb_armhf_limpo5.svg}{Intervalo 5 do uso de hardware do container  MariaDB na arquitetura armhf}%
%\apendicesvg{todos fracionada/uso do container container_postgres_amd_limpo5.svg}{Intervalo 5 do uso de hardware do container Postgres na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_postgres_armhf_limpo5.svg}{Intervalo 5 do uso de hardware do container Postgres na arquitetura armhf}%
%\pagebreak

%\subsec{Resultados todos parte 6}
%\apendicesvg{todos fracionada/uso do container container_mariadb_amd_limpo6.svg}{Intervalo 6 do uso de hardware do container  MariaDB na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_mariadb_armhf_limpo6.svg}{Intervalo 6 do uso de hardware do container  MariaDB na arquitetura armhf}%
%\apendicesvg{todos fracionada/uso do container container_postgres_amd_limpo6.svg}{Intervalo 6 do uso de hardware do container Postgres na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_postgres_armhf_limpo6.svg}{Intervalo 6 do uso de hardware do container Postgres na arquitetura armhf}%
%\pagebreak

%\subsec{Resultados todos parte 7}
%\apendicesvg{todos fracionada/uso do container container_mariadb_amd_limpo7.svg}{Intervalo 7 do uso de hardware do container  MariaDB na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_mariadb_armhf_limpo7.svg}{Intervalo 7 do uso de hardware do container  MariaDB na arquitetura armhf}%
%\apendicesvg{todos fracionada/uso do container container_postgres_amd_limpo7.svg}{Intervalo 7 do uso de hardware do container Postgres na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_postgres_armhf_limpo7.svg}{Intervalo 7 do uso de hardware do container Postgres na arquitetura armhf}%
%\pagebreak

%\subsec{Resultados todos parte 8}
%\apendicesvg{todos fracionada/uso do container container_mariadb_amd_limpo8.svg}{Intervalo 8 do uso de hardware do container  MariaDB na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_mariadb_armhf_limpo8.svg}{Intervalo 8 do uso de hardware do container  MariaDB na arquitetura armhf}%
%\apendicesvg{todos fracionada/uso do container container_postgres_amd_limpo8.svg}{Intervalo 8 do uso de hardware do container Postgres na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_postgres_armhf_limpo8.svg}{Intervalo 8 do uso de hardware do container Postgres na arquitetura armhf}%
%\pagebreak

%\subsec{Resultados todos parte 9}
%\apendicesvg{todos fracionada/uso do container container_mariadb_amd_limpo9.svg}{Intervalo 9 do uso de hardware do container  MariaDB na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_mariadb_armhf_limpo9.svg}{Intervalo 9 do uso de hardware do container  MariaDB na arquitetura armhf}%
%\apendicesvg{todos fracionada/uso do container container_postgres_amd_limpo9.svg}{Intervalo 9 do uso de hardware do container Postgres na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_postgres_armhf_limpo9.svg}{Intervalo 9 do uso de hardware do container Postgres na arquitetura armhf}%
%\pagebreak

%\subsec{Resultados todos parte 10}
%\apendicesvg{todos fracionada/uso do container container_mariadb_amd_limpo10.svg}{Intervalo 10 do uso de hardware do container  MariaDB na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_mariadb_armhf_limpo10.svg}{Intervalo 10 do uso de hardware do container  MariaDB na arquitetura armhf}%
%\apendicesvg{todos fracionada/uso do container container_postgres_amd_limpo10.svg}{Intervalo 10 do uso de hardware do container Postgres na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_postgres_armhf_limpo10.svg}{Intervalo 10 do uso de hardware do container Postgres na arquitetura armhf}%
%\pagebreak

%\subsec{Resultados todos parte 11}
%\apendicesvg{todos fracionada/uso do container container_mariadb_amd_limpo11.svg}{Intervalo 11 do uso de hardware do container  MariaDB na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_mariadb_armhf_limpo11.svg}{Intervalo 11 do uso de hardware do container  MariaDB na arquitetura armhf}%
%\apendicesvg{todos fracionada/uso do container container_postgres_amd_limpo11.svg}{Intervalo 11 do uso de hardware do container Postgres na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_postgres_armhf_limpo11.svg}{Intervalo 11 do uso de hardware do container Postgres na arquitetura armhf}%
%\pagebreak

%\subsec{Resultados todos parte 12}
%\apendicesvg{todos fracionada/uso do container container_mariadb_amd_limpo12.svg}{Intervalo 12 do uso de hardware do container  MariaDB na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_mariadb_armhf_limpo12.svg}{Intervalo 12 do uso de hardware do container  MariaDB na arquitetura armhf}%
%\apendicesvg{todos fracionada/uso do container container_postgres_amd_limpo12.svg}{Intervalo 12 do uso de hardware do container Postgres na arquitetura amd64}%
%\apendicesvg{todos fracionada/uso do container container_postgres_armhf_limpo12.svg}{Intervalo 12 do uso de hardware do container Postgres na arquitetura armhf}%
%\pagebreak

\sec{Arquivos}
\subsec{Logs todas operações  MariaDB amd}
\fbox{\begin{minipage}{\textwidth}

uso de RAM\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 2, 2, 0, 15, 54]\newline
[6362, 50779, 216701, 273480, 32996, 555223, 99182, 3766, 341, 17]\newline
[22, 19, 5, 2, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de RAM é de 40\newline
o valor com mais ocorrencias de uso de RAM é 35\newline
\newline
\newline
\newline
uso de CPU\newline
[4, 7, 2, 1, 0, 1, 5, 2482, 51165, 700744]\newline
[0, 1288, 49, 11, 86, 3079, 290263, 2, 59534, 0]\newline
[0, 246, 682, 9325, 0, 10315, 30, 9474, 334, 0]\newline
[1448, 1, 0, 4629, 0, 142, 4702, 25, 738, 0]\newline
[0, 2022, 56, 5, 321, 2814, 383, 1, 0, 0]\newline
[0, 0, 1, 341, 1805, 181, 1, 80, 926, 0]\newline
[0, 237, 6, 1426, 55, 0, 762, 0, 2, 114]\newline
[0, 8, 1037, 1, 0, 487, 82, 83, 8, 0]\newline
[0, 856, 0, 463, 90, 7, 0, 2, 50, 0]\newline
[943, 647, 153, 2, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de CPU é de 10\newline
o valor com mais ocorrencias de uso de CPU é 09\newline
\end{minipage}}
%\pagebreak

\subsec{Logs todos operações  MariaDB armhf}
\fbox{\begin{minipage}{\textwidth}
uso de RAM\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 5, 16, 8]\newline
[16, 21, 58, 123, 903, 7033, 21051, 106000, 133555, 188308]\newline
[240178, 33226, 110, 31, 45, 21, 74, 106, 22, 131]\newline
[70, 7, 94, 4, 131, 52, 13, 276, 3, 168]\newline
[369, 19, 1, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de RAM é de 30\newline
o valor com mais ocorrencias de uso de RAM é 30\newline
\newline
\newline
\newline
uso de CPU\newline
[21, 6, 7, 12, 15, 72, 129, 65494, 34474, 104121]\newline
[5, 252, 96, 132, 2292, 7845, 385212, 10, 30660, 5]\newline
[4, 1269, 188, 189609, 3, 22745, 145, 4737, 6845, 16]\newline
[5311, 41, 0, 7238, 0, 816, 3030, 23, 3269, 0]\newline
[1, 4067, 585, 12, 18, 1582, 2072, 6, 0, 0]\newline
[0, 0, 2, 1352, 1081, 18, 3, 239, 1733, 0]\newline
[0, 881, 1, 725, 174, 0, 1187, 0, 5, 646]\newline
[1, 123, 458, 12, 0, 802, 519, 5, 95, 0]\newline
[0, 227, 0, 433, 261, 77, 11, 2, 2, 0]\newline
[102, 215, 145, 3, 1, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de CPU é de 20\newline
o valor com mais ocorrencias de uso de CPU é 16\newline
\end{minipage}}
%\pagebreak

\subsec{Logs todos operações Postgres amd}
\fbox{\begin{minipage}{\textwidth}
 uso de RAM\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 3263, 30386, 26906, 1113, 12273, 13977, 6987]\newline
[12137, 12152, 14554, 16969, 24602, 32591, 34546, 26028, 29965, 35166]\newline
[37614, 35360, 36454, 39982, 44499, 51398, 54360, 61957, 64734, 70274]\newline
[73288, 71273, 74495, 71518, 69823, 62375, 54838, 46323, 40031, 42772]\newline
[55185, 72958, 82168, 73553, 37296, 6388, 691, 523, 321, 93]\newline
[15, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de RAM é de 60\newline
o valor com mais ocorrencias de uso de RAM é 62\newline
\newline
\newline
\newline
uso de CPU\newline
[6, 8, 1, 3, 1, 1, 31, 2597, 38173, 344957]\newline
[0, 17264, 1282, 143, 901, 7344, 166851, 17, 177235, 1]\newline
[0, 1847, 13955, 19279, 1, 94879, 784, 146286, 3954, 65]\newline
[23579, 346, 3, 101781, 1, 6089, 124456, 2092, 25778, 0]\newline
[3, 79642, 7366, 1016, 11027, 102958, 27511, 663, 2, 1]\newline
[1, 1, 891, 26264, 84182, 9902, 1478, 8258, 57722, 38]\newline
[18, 20908, 3810, 65989, 7925, 204, 60423, 14, 1976, 18270]\newline
[927, 7100, 55495, 3289, 15, 44659, 17674, 9914, 6742, 61]\newline
[35, 45854, 1037, 33769, 14665, 6772, 2917, 4874, 12472, 378]\newline
[45469, 37198, 23804, 5302, 2657, 991, 261, 52, 2, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de CPU é de 10\newline
o valor com mais ocorrencias de uso de CPU é 09\newline
\end{minipage}}
%\pagebreak

\subsec{Logs todos operações Postgres armhf}
\fbox{\begin{minipage}{\textwidth}
 uso de RAM\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 255, 7152, 16283]\newline
[32831, 19896, 2336, 2662, 4372, 6181, 7587, 12039, 10345, 12301]\newline
[15140, 20210, 21261, 22132, 25138, 25781, 24380, 24325, 25114, 28020]\newline
[33401, 32594, 36043, 34009, 37117, 38893, 37737, 39704, 44899, 44957]\newline
[44297, 46854, 46327, 49719, 51753, 48376, 52095, 58927, 59520, 68535]\newline
[73293, 80115, 79706, 61333, 40287, 26496, 17648, 11358, 6472, 2002]\newline
[577, 220, 94, 22, 80, 29, 4, 51, 100, 59]\newline
[72, 55, 24, 12, 3, 27, 121, 72, 70, 61]\newline
[71, 14, 7, 2, 2, 3, 1, 83, 61, 112]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de RAM é de 60\newline
o valor com mais ocorrencias de uso de RAM é 61\newline
\newline
\newline
\newline
uso de CPU\newline
[6, 12, 10, 28, 68, 588, 3618, 118494, 330874, 352961]\newline
[85, 10533, 4960, 2176, 16008, 92217, 623653, 285, 282050, 39]\newline
[7, 12853, 7361, 220074, 4, 274752, 2669, 134714, 22195, 439]\newline
[89115, 1320, 2, 169013, 19, 15285, 93588, 4316, 64198, 23]\newline
[36, 117739, 14214, 1406, 4456, 70651, 54517, 903, 19, 1]\newline
[0, 10, 934, 47887, 58121, 3592, 1484, 11312, 81406, 91]\newline
[69, 38937, 2630, 46263, 10104, 218, 75911, 31, 1597, 36127]\newline
[632, 9434, 43290, 3032, 46, 69151, 34191, 2579, 8050, 88]\newline
[52, 33862, 653, 46121, 23382, 6358, 2337, 1375, 1741, 188]\newline
[25154, 40896, 30033, 3361, 890, 339, 148, 70, 8, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de CPU é de 20\newline
o valor com mais ocorrencias de uso de CPU é 16\newline
\end{minipage}}
%\pagebreak

\ch{Apendices relacionados ao teste insercao}
\sec{Imagens}
primeira linha dos resultados mostra o uso deCPU,na segunda mostra o uso de RAM e na terceira mostra o uso de disco
\subsec{Todos tempos}
%%\apendicesvg{insercao fracionada/insercao tempos.svg}{valores tempo benchmark de operação de inserção,o eixo x corresponde a quantidade de elementos totais inseridos em função de 10e6  e o eixo y corresponde ao tempo gasto em segundos para cada 5000 elementos inseridos}
\subsec{Resultados insercao parte 0}
\apendicesvg{insercao fracionada/uso do container container_mariadb_amd_limpo0.svg}{Intervalo 1 do uso de hardware do container  MariaDB na arquitetura amd64}
\apendicesvg{insercao fracionada/uso do container container_mariadb_armhf_limpo0.svg}{Intervalo 1 do uso de hardware do container  MariaDB na arquitetura armhf}
\apendicesvg{insercao fracionada/uso do container container_postgres_amd_limpo0.svg}{Intervalo 1 do uso de hardware do container Postgres na arquitetura amd64}
\apendicesvg{insercao fracionada/uso do container container_postgres_armhf_limpo0.svg}{Intervalo 1 do de hardware do container Postgres na arquitetura amd64}
%\pagebreak

\sec{Arquivos}
\subsec{Logs insercao  MariaDB amd}
\fbox{\begin{minipage}{\textwidth}
uso de RAM\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 2, 2, 0, 15, 54]\newline
[6362, 50779, 216701, 273480, 32996, 555223, 99182, 3766, 341, 17]\newline
[22, 19, 5, 2, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de RAM é de 40\newline
o valor com mais ocorrencias de uso de RAM é 35\newline
\newline
\newline
\newline
uso de CPU\newline
[4, 7, 2, 1, 0, 1, 5, 2482, 51165, 700744]\newline
[0, 1288, 49, 11, 86, 3079, 290263, 2, 59534, 0]\newline
[0, 246, 682, 9325, 0, 10315, 30, 9474, 334, 0]\newline
[1448, 1, 0, 4629, 0, 142, 4702, 25, 738, 0]\newline
[0, 2022, 56, 5, 321, 2814, 383, 1, 0, 0]\newline
[0, 0, 1, 341, 1805, 181, 1, 80, 926, 0]\newline
[0, 237, 6, 1426, 55, 0, 762, 0, 2, 114]\newline
[0, 8, 1037, 1, 0, 487, 82, 83, 8, 0]\newline
[0, 856, 0, 463, 90, 7, 0, 2, 50, 0]\newline
[943, 647, 153, 2, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de CPU é de 10\newline
o valor com mais ocorrencias de uso de CPU é 09\newline
\end{minipage}}
%\pagebreak

\subsec{Logs insercao  MariaDB armhf}
\fbox{\begin{minipage}{\textwidth}
uso de RAM\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 5, 16, 8]\newline
[16, 21, 58, 123, 903, 7033, 21051, 106000, 133555, 188308]\newline
[240178, 33226, 110, 31, 45, 21, 74, 106, 22, 131]\newline
[70, 7, 94, 4, 131, 52, 13, 276, 3, 168]\newline
[369, 19, 1, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de RAM é de 30\newline
o valor com mais ocorrencias de uso de RAM é 30\newline
\newline
\newline
\newline
uso de CPU\newline
[21, 6, 7, 12, 15, 72, 129, 65494, 34474, 104121]\newline
[5, 252, 96, 132, 2292, 7845, 385212, 10, 30660, 5]\newline
[4, 1269, 188, 189609, 3, 22745, 145, 4737, 6845, 16]\newline
[5311, 41, 0, 7238, 0, 816, 3030, 23, 3269, 0]\newline
[1, 4067, 585, 12, 18, 1582, 2072, 6, 0, 0]\newline
[0, 0, 2, 1352, 1081, 18, 3, 239, 1733, 0]\newline
[0, 881, 1, 725, 174, 0, 1187, 0, 5, 646]\newline
[1, 123, 458, 12, 0, 802, 519, 5, 95, 0]\newline
[0, 227, 0, 433, 261, 77, 11, 2, 2, 0]\newline
[102, 215, 145, 3, 1, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de CPU é de 20\newline
o valor com mais ocorrencias de uso de CPU é 16\newline
\end{minipage}}
\pagebreak

\subsec{Logs insercao Postgres amd}
\fbox{\begin{minipage}{\textwidth}
uso de RAM\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 3263, 30386, 26906, 1113, 12273, 13977, 6987]\newline
[12137, 12152, 14554, 16969, 24602, 32591, 34546, 26028, 29965, 35166]\newline
[37614, 35360, 36454, 39982, 44499, 51398, 54360, 61957, 64734, 70274]\newline
[73288, 71273, 74495, 71518, 69823, 62375, 54838, 46323, 40031, 42772]\newline
[55185, 72958, 82168, 73553, 37296, 6388, 691, 523, 321, 93]\newline
[15, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de RAM é de 60\newline
o valor com mais ocorrencias de uso de RAM é 62\newline
\newline
\newline
\newline
uso de CPU\newline
[6, 8, 1, 3, 1, 1, 31, 2597, 38173, 344957]\newline
[0, 17264, 1282, 143, 901, 7344, 166851, 17, 177235, 1]\newline
[0, 1847, 13955, 19279, 1, 94879, 784, 146286, 3954, 65]\newline
[23579, 346, 3, 101781, 1, 6089, 124456, 2092, 25778, 0]\newline
[3, 79642, 7366, 1016, 11027, 102958, 27511, 663, 2, 1]\newline
[1, 1, 891, 26264, 84182, 9902, 1478, 8258, 57722, 38]\newline
[18, 20908, 3810, 65989, 7925, 204, 60423, 14, 1976, 18270]\newline
[927, 7100, 55495, 3289, 15, 44659, 17674, 9914, 6742, 61]\newline
[35, 45854, 1037, 33769, 14665, 6772, 2917, 4874, 12472, 378]\newline
[45469, 37198, 23804, 5302, 2657, 991, 261, 52, 2, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de CPU é de 10\newline
o valor com mais ocorrencias de uso de CPU é 09\newline
\end{minipage}}
\pagebreak

\subsec{Logs insercao Postgres armhf}
\fbox{\begin{minipage}{\textwidth}
uso de RAM\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 255, 7152, 16283]\newline
[32831, 19896, 2336, 2662, 4372, 6181, 7587, 12039, 10345, 12301]\newline
[15140, 20210, 21261, 22132, 25138, 25781, 24380, 24325, 25114, 28020]\newline
[33401, 32594, 36043, 34009, 37117, 38893, 37737, 39704, 44899, 44957]\newline
[44297, 46854, 46327, 49719, 51753, 48376, 52095, 58927, 59520, 68535]\newline
[73293, 80115, 79706, 61333, 40287, 26496, 17648, 11358, 6472, 2002]\newline
[577, 220, 94, 22, 80, 29, 4, 51, 100, 59]\newline
[72, 55, 24, 12, 3, 27, 121, 72, 70, 61]\newline
[71, 14, 7, 2, 2, 3, 1, 83, 61, 112]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de RAM é de 60\newline
o valor com mais ocorrencias de uso de RAM é 61\newline
\newline
\newline
\newline
uso de CPU\newline
[6, 12, 10, 28, 68, 588, 3618, 118494, 330874, 352961]\newline
[85, 10533, 4960, 2176, 16008, 92217, 623653, 285, 282050, 39]\newline
[7, 12853, 7361, 220074, 4, 274752, 2669, 134714, 22195, 439]\newline
[89115, 1320, 2, 169013, 19, 15285, 93588, 4316, 64198, 23]\newline
[36, 117739, 14214, 1406, 4456, 70651, 54517, 903, 19, 1]\newline
[0, 10, 934, 47887, 58121, 3592, 1484, 11312, 81406, 91]\newline
[69, 38937, 2630, 46263, 10104, 218, 75911, 31, 1597, 36127]\newline
[632, 9434, 43290, 3032, 46, 69151, 34191, 2579, 8050, 88]\newline
[52, 33862, 653, 46121, 23382, 6358, 2337, 1375, 1741, 188]\newline
[25154, 40896, 30033, 3361, 890, 339, 148, 70, 8, 0]\newline
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\newline
a faixa com mais ocorrencias de uso de CPU é de 20\newline
o valor com mais ocorrencias de uso de CPU é 16\newline
\end{minipage}}
\postextual


% ---------------------------------------------------------------------------------------------
% Referências bibliográficas
% ---------------------------------------------------------------------------------------------

% ---------------------------------------------------------------------------------------------
% Glossário
% ---------------------------------------------------------------------------------------------
%
% Consulte o manual da classe abntex2 para orientações sobre o glossário.
%
%\glossary

% ---------------------------------------------------------------------------------------------

% Anexos
% ---------------------------------------------------------------------------------------------

% ---
% Inicia os anexos
% ---

% ---------------------------------------------------------------------------------------------
% INDICE REMISSIVO
% ---------------------------------------------------------------------------------------------

\printindex

\end{document}
